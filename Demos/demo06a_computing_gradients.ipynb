{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo:  Computing Gradients\n",
    "\n",
    "Most numerical optimization methods require that we compute gradients of the loss function that we are attempting to minimize.  In this demo, we illustrate how to compute gradients efficiently in python for a few simple examples.  As much as possible, we avoid for loops for fast implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1:  A Simple Vector-Input Function\n",
    "\n",
    "Suppose `J(w) = w_0^2 + 2w_0w_1^3`.  Then the function and gradient at `w=[2,4]` can be computed as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J\t=260\n",
      "Jgrad\t=[132 192]\n"
     ]
    }
   ],
   "source": [
    "def Jeval(w):\n",
    "    \n",
    "    # Function\n",
    "    J = w[0]**2 + 2*w[0]*(w[1]**3)\n",
    "\n",
    "    # Gradient\n",
    "    dJ0 = 2*w[0]+2*(w[1]**3)\n",
    "    dJ1 = 6*w[0]*(w[1]**2)\n",
    "    Jgrad = np.array([dJ0, dJ1])\n",
    "    \n",
    "    return J, Jgrad\n",
    "\n",
    "# Point to evaluate \n",
    "w = np.array([2,4])\n",
    "J, Jgrad =  Jeval(w)\n",
    "\n",
    "print('J\\t={0}\\nJgrad\\t={1}'.format(J,Jgrad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2:  Non-Linear Least Squares for an Exponential Model\n",
    "\n",
    "Consider an exponential model \n",
    "\n",
    "    yhat = a*exp(-b*x)\n",
    "    \n",
    "for parameters `w=[a,b]`.  Given training data `(x[i],y[i])` a natural loss function is given by\n",
    "\n",
    "    J(w) := \\sum_i (y[i] - yhat[i])**2,   yhat[i] = a*exp(-b*x[i])\n",
    "    \n",
    "The following code computes the the loss function `J(w)` and its gradient `dJ/dw`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J\t=3835.653983662096\n",
      "Jgrad\t=[ 7849.48274896 13608.30853013]\n"
     ]
    }
   ],
   "source": [
    "def Jeval(y,x,w):\n",
    "    # Unpack vector\n",
    "    a = w[0]\n",
    "    b = w[1]\n",
    "\n",
    "    # Compute the loss function\n",
    "    yerr = y-a*np.exp(-b*x)\n",
    "    J = 0.5*np.sum(yerr**2)\n",
    "\n",
    "    # Compute the gradient\n",
    "    dJ_da = -np.sum( yerr*np.exp(-b*x))\n",
    "    dJ_db = np.sum( yerr*a*x*np.exp(-b*x))\n",
    "    Jgrad = np.array([dJ_da, dJ_db])\n",
    "    \n",
    "    return J,Jgrad\n",
    "\n",
    "# Generate some random data\n",
    "ny = 100\n",
    "y = np.random.randn(ny)\n",
    "x = np.random.rand(ny)\n",
    "\n",
    "# Set some arbitrary parameters \n",
    "w = np.array([1,2])\n",
    "\n",
    "# Evaluate cost and gradient \n",
    "J, Jgrad =  Jeval(x,y,w)\n",
    "print('J\\t={0}\\nJgrad\\t={1}'.format(J,Jgrad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3:  A Function of a Matrix.\n",
    "\n",
    "Suppose `f(W) = a'*W*b`.  Then, `fgrad(W) = a*b.T`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J\t=3.0519652510597286\n",
      "Jgrad\t=[[-0.60299933 -0.88827298  2.10413285]\n",
      " [ 0.23517239  0.34643037 -0.82062107]\n",
      " [-0.24342408 -0.35858586  0.84941489]\n",
      " [-0.10596333 -0.15609365  0.3697532 ]]\n"
     ]
    }
   ],
   "source": [
    "def Jeval(W,a,b):\n",
    "\n",
    "    # Function\n",
    "    J = a.dot(W.dot(b))\n",
    "\n",
    "    # Gradient -- Use python broadcasting\n",
    "    Jgrad = a[:,None]*b[None,:]\n",
    "    \n",
    "    return J,Jgrad\n",
    "\n",
    "# Some random data\n",
    "m = 4\n",
    "n = 3\n",
    "W = np.random.randn(m,n)\n",
    "a = np.random.randn(m)\n",
    "b = np.random.randn(n)\n",
    "\n",
    "J,Jgrad = Jeval(W,a,b)\n",
    "\n",
    "print('J\\t={0}\\nJgrad\\t={1}'.format(J,Jgrad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(a[:,None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
