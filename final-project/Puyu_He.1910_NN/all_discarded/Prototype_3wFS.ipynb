{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8b46637-09dd-4e48-846b-7a49ceb4af89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import r2_score\n",
    "from skorch import NeuralNetRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Lasso, Ridge, LinearRegression\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78dacaf1-6868-4ae5-befd-a1d4668bf8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "Xtr_loadpath = 'Xtr.csv'\n",
    "Xts_loadpath = 'Xts.csv'\n",
    "ytr_loadpath = 'ytr.csv'\n",
    "\n",
    "Xtr = np.loadtxt(Xtr_loadpath, delimiter=\",\")\n",
    "Xts = np.loadtxt(Xts_loadpath, delimiter=\",\")\n",
    "ytr = np.loadtxt(ytr_loadpath, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae45109c-c2ca-4718-869e-276b7140fdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize the training data\n",
    "Xtr_mean = np.mean(Xtr,axis=0)\n",
    "Xtr_std = np.std(Xtr,axis=0)\n",
    "ytr_mean = np.mean(ytr)\n",
    "ytr_std = np.std(ytr)\n",
    "\n",
    "Xtr_standardized = ((Xtr-Xtr_mean[None,:])/Xtr_std[None,:]) # revise this line as needed\n",
    "Xts_standardized = ((Xts-Xtr_mean[None,:])/Xtr_std[None,:]) # revise this line as needed\n",
    "ytr_standardized = ((ytr-ytr_mean)/ytr_std)\n",
    "\n",
    "# save the standardized training data\n",
    "Xtr_savepath = 'Xtr_pytorch.csv'\n",
    "Xts_savepath = 'Xts_pytorch.csv'\n",
    "ytr_savepath = 'ytr_pytorch.csv'\n",
    "yts_hat_savepath = 'yts_hat_pytorch.csv'\n",
    "\n",
    "np.savetxt(Xtr_savepath, Xtr_standardized, delimiter=\",\")\n",
    "np.savetxt(Xts_savepath, Xts_standardized, delimiter=\",\")\n",
    "np.savetxt(ytr_savepath, ytr_standardized, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6415030a-fa77-4335-b67f-dc03974848b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.2025827  -0.87100439 -0.69775652  2.91757736  0.         -1.68164859\n",
      "  4.71752763 -0.53762284  1.65814527  0.36276424 -0.77379783  4.68714744\n",
      "  0.87516852 -0.22619053  0.20134673  0.          0.         -0.38159997\n",
      "  2.08580527 -3.0914095   1.73222023 -5.09691452  0.03116045  0.11419508\n",
      "  0.02873892 -0.13480067]\n",
      "[21  6 11 19  3 18 20  5  8  0]\n"
     ]
    }
   ],
   "source": [
    "lasso = Lasso()\n",
    "lasso.set_params(alpha = 0.02, max_iter=10000, tol=0.01, warm_start=True)\n",
    "lasso.fit(Xtr_standardized,ytr)\n",
    "print(lasso.coef_)\n",
    "feature_selected = abs(lasso.coef_).argsort()[-10:][::-1]\n",
    "print(feature_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49ad9519-d44c-4559-9d5b-370953b7edaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 26)\n",
      "(10000, 26)\n",
      "(10000, 10)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(Xtr_standardized.shape)\n",
    "print(Xts_standardized.shape)\n",
    "Xtr_featSel = Xtr_standardized[:,feature_selected]\n",
    "Xts_featSel = Xts_standardized[:,feature_selected]\n",
    "print(Xtr_featSel.shape)\n",
    "print(Xts_featSel.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22b1cbed-ee8a-4089-9372-b214cff5f85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the numpy arrays to PyTorch tensors\n",
    "Xtr_torch = torch.Tensor(Xtr_featSel)\n",
    "ytr_torch = torch.Tensor(ytr)\n",
    "\n",
    "batch_size = 100  # size of each batch\n",
    "\n",
    "# Create a training Dataset\n",
    "train_ds = torch.utils.data.TensorDataset(Xtr_torch, ytr_torch)\n",
    "# Creates a training DataLoader from this Dataset\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a849753-8c0d-4901-9610-8127d3a23261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=10, out_features=2980, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Dropout(p=0.5, inplace=False)\n",
      "  (3): Linear(in_features=2980, out_features=1096, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Dropout(p=0.5, inplace=False)\n",
      "  (6): Linear(in_features=1096, out_features=403, bias=True)\n",
      "  (7): ReLU()\n",
      "  (8): Dropout(p=0.5, inplace=False)\n",
      "  (9): Linear(in_features=403, out_features=148, bias=True)\n",
      "  (10): ReLU()\n",
      "  (11): Linear(in_features=148, out_features=55, bias=True)\n",
      "  (12): ReLU()\n",
      "  (13): Linear(in_features=55, out_features=20, bias=True)\n",
      "  (14): ReLU()\n",
      "  (15): Linear(in_features=20, out_features=7, bias=True)\n",
      "  (16): ReLU()\n",
      "  (17): Linear(in_features=7, out_features=3, bias=True)\n",
      "  (18): ReLU()\n",
      "  (19): Linear(in_features=3, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "nin = Xtr_featSel.shape[1]\n",
    "nout = 1\n",
    "\n",
    "# model = nn.Sequential(\n",
    "#     nn.Linear(nin, 128*5),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Dropout(p=0.5),  # Add dropout layer with probability 0.2\n",
    "#     nn.Linear(128*5, 64*5),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Dropout(p=0.5),  # Add dropout layer with probability 0.2\n",
    "#     nn.Linear(64*5, 32*5),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Dropout(p=0.5),  # Add dropout layer with probability 0.2\n",
    "#     nn.Linear(32*5, 16*5),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Dropout(p=0.5),  # Add dropout layer with probability 0.2\n",
    "#     nn.Linear(16*5, 8*5),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(8*5, 4*5),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(4*5, 2*5),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(2*5, nout)\n",
    "# )\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(nin, 2980),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),  # Add dropout layer with probability 0.2\n",
    "    nn.Linear(2980, 1096),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),  # Add dropout layer with probability 0.2\n",
    "    nn.Linear(1096, 403),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),  # Add dropout layer with probability 0.2   \n",
    "    nn.Linear(403, 148),\n",
    "    nn.ReLU(),\n",
    "    #nn.Dropout(p=0.5),  # Add dropout layer with probability 0.2  \n",
    "    nn.Linear(148, 55),\n",
    "    nn.ReLU(),\n",
    "    #nn.Dropout(p=0.5),  # Add dropout layer with probability 0.2\n",
    "    nn.Linear(55, 20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 7),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(7, 3),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(3, nout)\n",
    ")\n",
    "\n",
    "print(str(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40f6e190-eefa-4e65-9755-9766c0ac95c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing the optimizer and loss function\n",
    "\n",
    "epochs = 1\n",
    "lrate = 1e-6\n",
    "decay = lrate/epochs\n",
    "lambda1 = lambda epoch: (1-decay)*epoch\n",
    "\n",
    "opt = optim.Adam(model.parameters(), lr=lrate)\n",
    "scheduler = optim.lr_scheduler.LambdaLR(opt, lr_lambda=lambda1)\n",
    "criterion = nn.MSELoss()\n",
    "#criterion = nn.HuberLoss(reduction='mean', delta=30)\n",
    "#criterion = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a53724c-0242-47c5-b0c1-7bfba9c13c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 10])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([100, 10])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([100, 10])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([100, 10])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([100, 10])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([100, 10])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([100, 10])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([100, 10])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([100, 10])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([100, 10])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([100, 10])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([100, 10])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([100, 10])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([100, 10])\n",
      "torch.Size([100, 1])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_32592\\1250459870.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mbatch_Rsq_tr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# iterate over training set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    632\u001b[0m                 \u001b[1;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 634\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    635\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    262\u001b[0m             \u001b[1;33m>>\u001b[0m\u001b[1;33m>\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Handle `CustomType` automatically\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m     \"\"\"\n\u001b[1;32m--> 264\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcollate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdefault_collate_fn_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcollate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# Backwards compatibility.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcollate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# Backwards compatibility.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcollate_fn_map\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0melem_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0melem_type\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcollate_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_typed_storage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "num_epoch = epochs\n",
    "\n",
    "a_tr_loss = np.zeros([num_epoch])\n",
    "a_tr_Rsq = np.zeros([num_epoch])\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "\n",
    "    model.train() # put model in training mode\n",
    "    batch_loss_tr = []\n",
    "    batch_Rsq_tr = []\n",
    "    # iterate over training set\n",
    "    for train_iter, data in enumerate(train_loader):\n",
    "        x_batch,y_batch = data\n",
    "        print(x_batch.shape)\n",
    "        y_batch = y_batch.view(-1,1)\n",
    "        #y_batch = y_batch.type(torch.long)\n",
    "        print(y_batch.shape)\n",
    "        out = model(x_batch)\n",
    "        # Compute Loss\n",
    "        loss = criterion(out,y_batch.type(torch.float))\n",
    "        batch_loss_tr.append(loss.item())\n",
    "        # Compute R-square\n",
    "        Rsq = r2_score(y_batch.type(torch.float).detach().numpy(), out.detach().numpy())\n",
    "        batch_Rsq_tr.append(Rsq.item())\n",
    "        # Compute gradients using back propagation\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        # Take an optimization 'step'\n",
    "        opt.step()\n",
    "        \n",
    "    # Take scheduler step\n",
    "    scheduler.step()\n",
    "        \n",
    "    a_tr_loss[epoch] = np.mean(batch_loss_tr) # Compute average loss over epoch\n",
    "    a_tr_Rsq[epoch] = np.mean(batch_Rsq_tr)\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print('Epoch: {0:2d}   Train Loss: {1:.3f}   '.format(epoch+1, a_tr_loss[epoch])\n",
    "             + 'R^2: {0:.3f}   '.format(a_tr_Rsq[epoch])\n",
    "             )\n",
    "    \n",
    "with torch.no_grad():\n",
    "    predict = model(torch.Tensor(Xtr_featSel)).detach().numpy().ravel()\n",
    "\n",
    "r2 = r2_score(ytr,predict)\n",
    "print('training R2: ',r2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286cf170-2380-4782-8ebe-a4d8d925b8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nepochs = len(a_tr_Rsq)\n",
    "plt.plot(np.arange(1,nepochs+1), a_tr_Rsq, 'o-', linewidth=1)\n",
    "plt.grid()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Train accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318343c0-4316-466a-b376-c8ecf89b262c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsteps = len(a_tr_loss)\n",
    "ntr = Xtr.shape[0]\n",
    "epochs = np.arange(1,nsteps+1)*batch_size/ntr\n",
    "plt.semilogy(epochs, a_tr_loss)\n",
    "plt.xlabel('Epochs', fontsize=16)\n",
    "plt.ylabel('Loss', fontsize=16)\n",
    "plt.grid()\n",
    "plt.xlim([0,np.max(epochs)])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebfbbc7-6ed4-4891-869c-65436200938d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xts_standardized = Xts_standardized[:,feature_selected]\n",
    "np.savetxt(Xtr_savepath, Xtr_featSel, delimiter=\",\")\n",
    "np.savetxt(Xts_savepath, Xts_featSel, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471376b7-d8f2-47e1-944e-89f48bad2861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model: you must use the .pth format for pytorch models!\n",
    "model_savepath = 'model.pth'\n",
    "\n",
    "# To save a PyTorch model, we first pass an input through the model, \n",
    "# and then save the \"trace\". \n",
    "# For this purpose, we can use any input. \n",
    "# We will create a random input with the proper dimension.\n",
    "x = torch.randn(10) # random input\n",
    "x = x[None,:] # add singleton batch index\n",
    "with torch.no_grad():\n",
    "    traced_cell = torch.jit.trace(model, (x))\n",
    "\n",
    "# Now we save the trace\n",
    "torch.jit.save(traced_cell, model_savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a961efc-47e6-4757-9bcb-3d590aa865e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate kaggle submission file using the validation script\n",
    "!python {\"validation.py \" + model_savepath + \" --Xts_path \" + Xts_savepath + \" --Xtr_path \" + Xtr_savepath + \" --yts_hat_path \" + yts_hat_savepath } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ba3c3a-aef0-441f-92dc-3c48da1b5937",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
