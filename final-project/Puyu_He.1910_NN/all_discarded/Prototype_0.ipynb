{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e8b46637-09dd-4e48-846b-7a49ceb4af89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import r2_score\n",
    "from skorch import NeuralNetRegressor\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78dacaf1-6868-4ae5-befd-a1d4668bf8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "Xtr_loadpath = 'Xtr.csv'\n",
    "Xts_loadpath = 'Xts.csv'\n",
    "ytr_loadpath = 'ytr.csv'\n",
    "\n",
    "Xtr = np.loadtxt(Xtr_loadpath, delimiter=\",\")\n",
    "Xts = np.loadtxt(Xts_loadpath, delimiter=\",\")\n",
    "ytr = np.loadtxt(ytr_loadpath, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae45109c-c2ca-4718-869e-276b7140fdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize the training data\n",
    "Xtr_mean = np.mean(Xtr,axis=0)\n",
    "Xtr_std = np.std(Xtr,axis=0)\n",
    "Xtr_standardized = ((Xtr-Xtr_mean[None,:])/Xtr_std[None,:]) # revise this line as needed\n",
    "Xts_standardized = ((Xts-Xtr_mean[None,:])/Xtr_std[None,:]) # revise this line as needed\n",
    "\n",
    "# save the standardized training data\n",
    "Xtr_savepath = 'Xtr_pytorch.csv'\n",
    "Xts_savepath = 'Xts_pytorch.csv'\n",
    "ytr_savepath = 'ytr_pytorch.csv'\n",
    "yts_hat_savepath = 'yts_hat_pytorch.csv'\n",
    "\n",
    "np.savetxt(Xtr_savepath, Xtr_standardized, delimiter=\",\")\n",
    "np.savetxt(Xts_savepath, Xts_standardized, delimiter=\",\")\n",
    "np.savetxt(ytr_savepath, ytr, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22b1cbed-ee8a-4089-9372-b214cff5f85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the numpy arrays to PyTorch tensors\n",
    "Xtr_torch = torch.Tensor(Xtr_standardized)\n",
    "ytr_torch = torch.Tensor(ytr)\n",
    "\n",
    "batch_size = 100  # size of each batch\n",
    "\n",
    "# Create a training Dataset\n",
    "train_ds = torch.utils.data.TensorDataset(Xtr_torch, ytr_torch)\n",
    "# Creates a training DataLoader from this Dataset\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9a849753-8c0d-4901-9610-8127d3a23261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=26, out_features=32, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=8, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "nin = Xtr.shape[1]\n",
    "nout = 1\n",
    "#nh = 256\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(nin, 32),\n",
    "    nn.ReLU(),\n",
    "    #nn.Linear(64, 32, bias=False),\n",
    "    #nn.ReLU(),\n",
    "    nn.Linear(32, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16, 8),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(8, nout)\n",
    ")\n",
    "\n",
    "print(str(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ba52800f-94bc-46c1-bb84-3dce3b557798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Hyper Parameter Gird Search Cross-Validation\n",
    "\n",
    "# import sklearn.model_selection \n",
    "\n",
    "# # construct repeated k-fold cross-val object\n",
    "# nfold = 10\n",
    "# nrep = 5\n",
    "# rkf = sklearn.model_selection.RepeatedKFold(n_splits=nfold,\n",
    "#                                             n_repeats=nrep, random_state=1)\n",
    "\n",
    "# # Define your PyTorch neural network as a class\n",
    "# class gscvNet(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(gscvNet,self).__init__()\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.hl1 = nn.Linear(26, 32)\n",
    "#         self.hl2 = nn.Linear(32, 16)\n",
    "#         self.hl3 = nn.Linear(16, 8)\n",
    "#         self.hl4 = nn.Linear(8, 1)\n",
    "        \n",
    "#     def forward(self,x):        \n",
    "#         x = self.hl1(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.hl2(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.hl3(x)\n",
    "#         x = self.relu(x)\n",
    "#         out = self.hl4(x)\n",
    "#         #return x        \n",
    "#         return out\n",
    "\n",
    "# # Create a dictionary of hyperparameters and their possible values\n",
    "# ##nh1_space = np.linspace(20,60,1)\n",
    "# ##nh2_space = np.linspace(10,40,1)\n",
    "# ##nh3_space = np.linspace(5,20,1)\n",
    "# criterion_space = [torch.nn.MSELoss, torch.nn.L1Loss, torch.nn.HuberLoss]\n",
    "\n",
    "# param_grid = {\n",
    "    \n",
    "#     'criterion': criterion_space\n",
    "#     #'optimizer': [optim.Adam, optim.SGD]\n",
    "#     #'optimizer_lr': [0.01, 0.001],\n",
    "# }\n",
    "\n",
    "# # Create a PyTorch neural network object using the above-defined class\n",
    "# cv_net = NeuralNetRegressor(\n",
    "#     gscvNet,\n",
    "#     max_epochs=25,\n",
    "#     #criterion=nn.MSELoss,\n",
    "# )\n",
    "\n",
    "# # Create a GridSearchCV object using the neural network and the hyperparameter dictionary\n",
    "# gscv = GridSearchCV(cv_net, param_grid, cv=rkf, scoring='r2')\n",
    "\n",
    "# # Fit the GridSearchCV object to your data\n",
    "# gscv.fit(Xtr_standardized.dtype(torch.long), ytr.reshape(-1, 1).dtype(torch.long))\n",
    "\n",
    "# # Print the best hyperparameters and the corresponding R-squared score\n",
    "# print(gs.best_params_, gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "40f6e190-eefa-4e65-9755-9766c0ac95c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing the optimizer and loss function\n",
    "\n",
    "opt = optim.Adam(model.parameters(), lr=0.01)\n",
    "#criterion = nn.MSELoss()\n",
    "criterion = nn.HuberLoss(reduction='mean', delta=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3a53724c-0242-47c5-b0c1-7bfba9c13c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1   Train Loss: 0.327   R^2: 0.338   \n",
      "Epoch:  2   Train Loss: 0.327   R^2: 0.333   \n",
      "Epoch:  3   Train Loss: 0.324   R^2: 0.346   \n",
      "Epoch:  4   Train Loss: 0.325   R^2: 0.356   \n",
      "Epoch:  5   Train Loss: 0.324   R^2: 0.352   \n",
      "Epoch:  6   Train Loss: 0.322   R^2: 0.362   \n",
      "Epoch:  7   Train Loss: 0.322   R^2: 0.348   \n",
      "Epoch:  8   Train Loss: 0.322   R^2: 0.352   \n",
      "Epoch:  9   Train Loss: 0.322   R^2: 0.356   \n",
      "Epoch: 10   Train Loss: 0.319   R^2: 0.370   \n",
      "Epoch: 11   Train Loss: 0.320   R^2: 0.369   \n",
      "Epoch: 12   Train Loss: 0.319   R^2: 0.360   \n",
      "Epoch: 13   Train Loss: 0.321   R^2: 0.363   \n",
      "Epoch: 14   Train Loss: 0.320   R^2: 0.351   \n",
      "Epoch: 15   Train Loss: 0.322   R^2: 0.371   \n",
      "Epoch: 16   Train Loss: 0.318   R^2: 0.384   \n",
      "Epoch: 17   Train Loss: 0.318   R^2: 0.368   \n",
      "Epoch: 18   Train Loss: 0.320   R^2: 0.371   \n",
      "Epoch: 19   Train Loss: 0.317   R^2: 0.375   \n",
      "Epoch: 20   Train Loss: 0.318   R^2: 0.368   \n",
      "Epoch: 21   Train Loss: 0.316   R^2: 0.383   \n",
      "Epoch: 22   Train Loss: 0.317   R^2: 0.372   \n",
      "Epoch: 23   Train Loss: 0.318   R^2: 0.388   \n",
      "Epoch: 24   Train Loss: 0.316   R^2: 0.386   \n",
      "Epoch: 25   Train Loss: 0.318   R^2: 0.365   \n",
      "Epoch: 26   Train Loss: 0.315   R^2: 0.373   \n",
      "Epoch: 27   Train Loss: 0.313   R^2: 0.381   \n",
      "Epoch: 28   Train Loss: 0.317   R^2: 0.389   \n",
      "Epoch: 29   Train Loss: 0.315   R^2: 0.379   \n",
      "Epoch: 30   Train Loss: 0.314   R^2: 0.391   \n",
      "Epoch: 31   Train Loss: 0.311   R^2: 0.399   \n",
      "Epoch: 32   Train Loss: 0.311   R^2: 0.398   \n",
      "Epoch: 33   Train Loss: 0.314   R^2: 0.386   \n",
      "Epoch: 34   Train Loss: 0.311   R^2: 0.393   \n",
      "Epoch: 35   Train Loss: 0.312   R^2: 0.407   \n",
      "Epoch: 36   Train Loss: 0.310   R^2: 0.398   \n",
      "Epoch: 37   Train Loss: 0.311   R^2: 0.394   \n",
      "Epoch: 38   Train Loss: 0.310   R^2: 0.409   \n",
      "Epoch: 39   Train Loss: 0.310   R^2: 0.397   \n",
      "Epoch: 40   Train Loss: 0.312   R^2: 0.389   \n",
      "Epoch: 41   Train Loss: 0.310   R^2: 0.403   \n",
      "Epoch: 42   Train Loss: 0.308   R^2: 0.410   \n",
      "Epoch: 43   Train Loss: 0.307   R^2: 0.411   \n",
      "Epoch: 44   Train Loss: 0.310   R^2: 0.401   \n",
      "Epoch: 45   Train Loss: 0.306   R^2: 0.403   \n",
      "Epoch: 46   Train Loss: 0.307   R^2: 0.397   \n",
      "Epoch: 47   Train Loss: 0.307   R^2: 0.412   \n",
      "Epoch: 48   Train Loss: 0.308   R^2: 0.401   \n",
      "Epoch: 49   Train Loss: 0.305   R^2: 0.415   \n",
      "Epoch: 50   Train Loss: 0.306   R^2: 0.407   \n",
      "Epoch: 51   Train Loss: 0.308   R^2: 0.405   \n",
      "Epoch: 52   Train Loss: 0.306   R^2: 0.422   \n",
      "Epoch: 53   Train Loss: 0.305   R^2: 0.411   \n",
      "Epoch: 54   Train Loss: 0.306   R^2: 0.413   \n",
      "Epoch: 55   Train Loss: 0.304   R^2: 0.420   \n",
      "Epoch: 56   Train Loss: 0.303   R^2: 0.421   \n",
      "Epoch: 57   Train Loss: 0.304   R^2: 0.422   \n",
      "Epoch: 58   Train Loss: 0.303   R^2: 0.427   \n",
      "Epoch: 59   Train Loss: 0.305   R^2: 0.413   \n",
      "Epoch: 60   Train Loss: 0.306   R^2: 0.413   \n",
      "Epoch: 61   Train Loss: 0.305   R^2: 0.417   \n",
      "Epoch: 62   Train Loss: 0.305   R^2: 0.418   \n",
      "Epoch: 63   Train Loss: 0.302   R^2: 0.428   \n",
      "Epoch: 64   Train Loss: 0.302   R^2: 0.432   \n",
      "Epoch: 65   Train Loss: 0.304   R^2: 0.417   \n",
      "Epoch: 66   Train Loss: 0.300   R^2: 0.432   \n",
      "Epoch: 67   Train Loss: 0.303   R^2: 0.418   \n",
      "Epoch: 68   Train Loss: 0.301   R^2: 0.425   \n",
      "Epoch: 69   Train Loss: 0.302   R^2: 0.428   \n",
      "Epoch: 70   Train Loss: 0.303   R^2: 0.415   \n",
      "Epoch: 71   Train Loss: 0.302   R^2: 0.416   \n",
      "Epoch: 72   Train Loss: 0.301   R^2: 0.428   \n",
      "Epoch: 73   Train Loss: 0.301   R^2: 0.436   \n",
      "Epoch: 74   Train Loss: 0.300   R^2: 0.426   \n",
      "Epoch: 75   Train Loss: 0.299   R^2: 0.436   \n",
      "Epoch: 76   Train Loss: 0.299   R^2: 0.434   \n",
      "Epoch: 77   Train Loss: 0.299   R^2: 0.433   \n",
      "Epoch: 78   Train Loss: 0.300   R^2: 0.426   \n",
      "Epoch: 79   Train Loss: 0.297   R^2: 0.430   \n",
      "Epoch: 80   Train Loss: 0.299   R^2: 0.435   \n",
      "Epoch: 81   Train Loss: 0.297   R^2: 0.446   \n",
      "Epoch: 82   Train Loss: 0.299   R^2: 0.435   \n",
      "Epoch: 83   Train Loss: 0.298   R^2: 0.443   \n",
      "Epoch: 84   Train Loss: 0.297   R^2: 0.430   \n",
      "Epoch: 85   Train Loss: 0.297   R^2: 0.443   \n",
      "Epoch: 86   Train Loss: 0.298   R^2: 0.443   \n",
      "Epoch: 87   Train Loss: 0.295   R^2: 0.443   \n",
      "Epoch: 88   Train Loss: 0.295   R^2: 0.439   \n",
      "Epoch: 89   Train Loss: 0.297   R^2: 0.436   \n",
      "Epoch: 90   Train Loss: 0.298   R^2: 0.445   \n",
      "Epoch: 91   Train Loss: 0.299   R^2: 0.432   \n",
      "Epoch: 92   Train Loss: 0.296   R^2: 0.444   \n",
      "Epoch: 93   Train Loss: 0.294   R^2: 0.447   \n",
      "Epoch: 94   Train Loss: 0.298   R^2: 0.444   \n",
      "Epoch: 95   Train Loss: 0.296   R^2: 0.444   \n",
      "Epoch: 96   Train Loss: 0.296   R^2: 0.450   \n",
      "Epoch: 97   Train Loss: 0.298   R^2: 0.432   \n",
      "Epoch: 98   Train Loss: 0.295   R^2: 0.448   \n",
      "Epoch: 99   Train Loss: 0.294   R^2: 0.450   \n",
      "Epoch: 100   Train Loss: 0.293   R^2: 0.443   \n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "num_epoch = 100\n",
    "\n",
    "a_tr_loss = np.zeros([num_epoch])\n",
    "a_tr_Rsq = np.zeros([num_epoch])\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "\n",
    "    model.train() # put model in training mode\n",
    "    batch_loss_tr = []\n",
    "    batch_Rsq_tr = []\n",
    "    # iterate over training set\n",
    "    for train_iter, data in enumerate(train_loader):\n",
    "        x_batch,y_batch = data\n",
    "        \n",
    "        y_batch = y_batch.view(-1,1)\n",
    "        #y_batch = y_batch.type(torch.long)\n",
    "        \n",
    "        out = model(x_batch)\n",
    "        # Compute Loss\n",
    "        loss = criterion(out,y_batch.type(torch.float))\n",
    "        batch_loss_tr.append(loss.item())\n",
    "        # Compute R-square\n",
    "        Rsq = r2_score(y_batch.detach().numpy(), out.detach().numpy())\n",
    "        batch_Rsq_tr.append(Rsq.item())\n",
    "        # Compute gradients using back propagation\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        # Take an optimization 'step'\n",
    "        opt.step()\n",
    "        \n",
    "    a_tr_loss[epoch] = np.mean(batch_loss_tr) # Compute average loss over epoch\n",
    "    a_tr_Rsq[epoch] = np.mean(batch_Rsq_tr)\n",
    "    print('Epoch: {0:2d}   Train Loss: {1:.3f}   '.format(epoch+1, a_tr_loss[epoch])\n",
    "         + 'R^2: {0:.3f}   '.format(a_tr_Rsq[epoch])\n",
    "         )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "471376b7-d8f2-47e1-944e-89f48bad2861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the model: you must use the .pth format for pytorch models!\n",
    "# model_savepath = 'model.pth'\n",
    "\n",
    "# # To save a PyTorch model, we first pass an input through the model, \n",
    "# # and then save the \"trace\". \n",
    "# # For this purpose, we can use any input. \n",
    "# # We will create a random input with the proper dimension.\n",
    "# x = torch.randn(26) # random input\n",
    "# x = x[None,:] # add singleton batch index\n",
    "# with torch.no_grad():\n",
    "#     traced_cell = torch.jit.trace(model, (x))\n",
    "\n",
    "# # Now we save the trace\n",
    "# torch.jit.save(traced_cell, model_savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "2a961efc-47e6-4757-9bcb-3d590aa865e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training R2 =  0.44558000919820573\n",
      "test target predictions saved in yts_hat_pytorch.csv\n"
     ]
    }
   ],
   "source": [
    "# # generate kaggle submission file using the validation script\n",
    "# !python {\"validation.py \" + model_savepath + \" --Xts_path \" + Xts_savepath + \" --Xtr_path \" + Xtr_savepath + \" --yts_hat_path \" + yts_hat_savepath } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc9a679-65b3-4923-9f78-6324169de84f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
