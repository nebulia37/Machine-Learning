{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8b46637-09dd-4e48-846b-7a49ceb4af89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import r2_score\n",
    "from skorch import NeuralNetRegressor\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78dacaf1-6868-4ae5-befd-a1d4668bf8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "Xtr_loadpath = 'Xtr.csv'\n",
    "Xts_loadpath = 'Xts.csv'\n",
    "ytr_loadpath = 'ytr.csv'\n",
    "\n",
    "Xtr = np.loadtxt(Xtr_loadpath, delimiter=\",\")\n",
    "Xts = np.loadtxt(Xts_loadpath, delimiter=\",\")\n",
    "ytr = np.loadtxt(ytr_loadpath, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae45109c-c2ca-4718-869e-276b7140fdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize the training data\n",
    "Xtr_mean = np.mean(Xtr,axis=0)\n",
    "Xtr_std = np.std(Xtr,axis=0)\n",
    "ytr_mean = np.mean(ytr)\n",
    "ytr_std = np.std(ytr)\n",
    "\n",
    "Xtr_standardized = ((Xtr-Xtr_mean[None,:])/Xtr_std[None,:]) # revise this line as needed\n",
    "Xts_standardized = ((Xts-Xtr_mean[None,:])/Xtr_std[None,:]) # revise this line as needed\n",
    "ytr_standardized = ((ytr-ytr_mean)/ytr_std)\n",
    "\n",
    "# save the standardized training data\n",
    "Xtr_savepath = 'Xtr_pytorch.csv'\n",
    "Xts_savepath = 'Xts_pytorch.csv'\n",
    "ytr_savepath = 'ytr_pytorch.csv'\n",
    "yts_hat_savepath = 'yts_hat_pytorch.csv'\n",
    "\n",
    "np.savetxt(Xtr_savepath, Xtr_standardized, delimiter=\",\")\n",
    "np.savetxt(Xts_savepath, Xts_standardized, delimiter=\",\")\n",
    "np.savetxt(ytr_savepath, ytr_standardized, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f8624d1-9de3-4831-b7dd-577d5445b4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # feature selection\n",
    "\n",
    "# Xtr_feat_sel = Xtr_standardized\n",
    "# Xtr_feat_sel[:,1] = np.ones(Xtr_feat_sel.shape[0])*1e-5\n",
    "# Xtr_feat_sel[:,3] = np.ones(Xtr_feat_sel.shape[0])*1e-5\n",
    "# Xtr_feat_sel[:,4] = np.ones(Xtr_feat_sel.shape[0])*1e-5\n",
    "# Xtr_feat_sel[:,7] = np.ones(Xtr_feat_sel.shape[0])*1e-5\n",
    "# Xtr_feat_sel[:,10] = np.ones(Xtr_feat_sel.shape[0])*1e-5\n",
    "# Xtr_feat_sel[:,14] = np.ones(Xtr_feat_sel.shape[0])*1e-5\n",
    "# Xtr_feat_sel[:,15] = np.ones(Xtr_feat_sel.shape[0])*1e-5\n",
    "# Xtr_feat_sel[:,16] = np.ones(Xtr_feat_sel.shape[0])*1e-5\n",
    "# Xtr_feat_sel[:,17] = np.ones(Xtr_feat_sel.shape[0])*1e-5\n",
    "# Xtr_feat_sel[:,18] = np.ones(Xtr_feat_sel.shape[0])*1e-5\n",
    "# Xtr_feat_sel[:,19] = np.ones(Xtr_feat_sel.shape[0])*1e-5\n",
    "# Xtr_feat_sel[:,22] = np.ones(Xtr_feat_sel.shape[0])*1e-5\n",
    "# Xtr_feat_sel[:,23] = np.ones(Xtr_feat_sel.shape[0])*1e-5\n",
    "# Xtr_feat_sel[:,24] = np.ones(Xtr_feat_sel.shape[0])*1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22b1cbed-ee8a-4089-9372-b214cff5f85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the numpy arrays to PyTorch tensors\n",
    "Xtr_torch = torch.Tensor(Xtr_standardized)\n",
    "ytr_torch = torch.Tensor(ytr)\n",
    "\n",
    "batch_size = 100  # size of each batch\n",
    "\n",
    "# Create a training Dataset\n",
    "train_ds = torch.utils.data.TensorDataset(Xtr_torch, ytr_torch)\n",
    "# Creates a training DataLoader from this Dataset\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d678c59-51c2-4e42-bbf2-8cce993defda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a model\n",
    "# d_in = Xtr.shape[1]\n",
    "# d_h = 100\n",
    "# d_out = 1\n",
    "\n",
    "# class NeuralNet(nn.Module):\n",
    "#     def __init__(self,din,dh1,dh2,dh3,dout):\n",
    "#         super(NeuralNet, self).__init__()\n",
    "#         self.Dense1 = nn.Linear(din,dh1)\n",
    "#         self.Dense2 = nn.Linear(dh1,dh2)\n",
    "#         self.Dense3 = nn.Linear(dh2,dh3)\n",
    "#         self.Dense4 = nn.Linear(dh3,dout)\n",
    "#         self.ReLU = nn.ReLU()\n",
    "        \n",
    "#     def forward(self,x):\n",
    "#         x = self.ReLU(self.Dense1(x))\n",
    "#         x = self.ReLU(self.Dense2(x))        \n",
    "#         x = self.ReLU(self.Dense3(x))        \n",
    "#         out = self.Dense4(x)\n",
    "#         return out\n",
    "\n",
    "# model = NeuralNet(din=d_in, dh1=d_h, dh2=64*2, dh3=30*2, dout=d_out)\n",
    "\n",
    "# print(str(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a849753-8c0d-4901-9610-8127d3a23261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=26, out_features=729, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Dropout(p=0.5, inplace=False)\n",
      "  (3): Linear(in_features=729, out_features=243, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Dropout(p=0.5, inplace=False)\n",
      "  (6): Linear(in_features=243, out_features=81, bias=True)\n",
      "  (7): ReLU()\n",
      "  (8): Linear(in_features=81, out_features=27, bias=True)\n",
      "  (9): ReLU()\n",
      "  (10): Linear(in_features=27, out_features=9, bias=True)\n",
      "  (11): ReLU()\n",
      "  (12): Linear(in_features=9, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "nin = Xtr.shape[1]\n",
    "nout = 1\n",
    "#nh = 256\n",
    "\n",
    "# model = nn.Sequential(\n",
    "#     nn.Linear(nin, 128*5),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Dropout(p=0.5),  # Add dropout layer with probability 0.2\n",
    "#     nn.Linear(128*5, 64*5),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Dropout(p=0.5),  # Add dropout layer with probability 0.2\n",
    "#     nn.Linear(64*5, 32*5),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Dropout(p=0.5),  # Add dropout layer with probability 0.2\n",
    "#     nn.Linear(32*5, 16*5),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Dropout(p=0.5),  # Add dropout layer with probability 0.2\n",
    "#     nn.Linear(16*5, 8*5),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(8*5, 4*5),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(4*5, 2*5),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(2*5, nout)\n",
    "# )\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(nin, 729),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),  # Add dropout layer with probability 0.2\n",
    "    #nn.Linear(2187, 729),\n",
    "    #nn.ReLU(),\n",
    "    #nn.Dropout(p=0.5),  # Add dropout layer with probability 0.2\n",
    "    nn.Linear(729, 243),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),  # Add dropout layer with probability 0.2\n",
    "    nn.Linear(243, 81),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(81, 27),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(27, 9),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(9, nout)\n",
    ")\n",
    "\n",
    "print(str(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40f6e190-eefa-4e65-9755-9766c0ac95c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing the optimizer and loss function\n",
    "\n",
    "epochs = 350\n",
    "lrate = 2.5e-6\n",
    "decay = lrate/epochs\n",
    "lambda1 = lambda epoch: (1-decay)*epoch\n",
    "\n",
    "opt = optim.Adam(model.parameters(), lr=lrate)\n",
    "scheduler = optim.lr_scheduler.LambdaLR(opt, lr_lambda=lambda1)\n",
    "criterion = nn.MSELoss()\n",
    "#criterion = nn.HuberLoss(reduction='mean', delta=0.6)\n",
    "#criterion = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a53724c-0242-47c5-b0c1-7bfba9c13c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1   Train Loss: 100.992   R^2: -0.010   \n",
      "Epoch:  2   Train Loss: 100.990   R^2: -0.012   \n",
      "Epoch:  3   Train Loss: 100.984   R^2: -0.013   \n",
      "Epoch:  4   Train Loss: 100.975   R^2: -0.011   \n",
      "Epoch:  5   Train Loss: 100.958   R^2: -0.012   \n",
      "Epoch:  6   Train Loss: 100.913   R^2: -0.013   \n",
      "Epoch:  7   Train Loss: 100.838   R^2: -0.010   \n",
      "Epoch:  8   Train Loss: 100.651   R^2: -0.012   \n",
      "Epoch:  9   Train Loss: 100.225   R^2: -0.004   \n",
      "Epoch: 10   Train Loss: 99.319   R^2: 0.005   \n",
      "Epoch: 11   Train Loss: 97.789   R^2: 0.017   \n",
      "Epoch: 12   Train Loss: 95.900   R^2: 0.033   \n",
      "Epoch: 13   Train Loss: 93.936   R^2: 0.053   \n",
      "Epoch: 14   Train Loss: 92.109   R^2: 0.075   \n",
      "Epoch: 15   Train Loss: 89.541   R^2: 0.101   \n",
      "Epoch: 16   Train Loss: 88.127   R^2: 0.111   \n",
      "Epoch: 17   Train Loss: 87.110   R^2: 0.127   \n",
      "Epoch: 18   Train Loss: 85.904   R^2: 0.141   \n",
      "Epoch: 19   Train Loss: 85.325   R^2: 0.139   \n",
      "Epoch: 20   Train Loss: 84.896   R^2: 0.155   \n",
      "Epoch: 21   Train Loss: 83.995   R^2: 0.160   \n",
      "Epoch: 22   Train Loss: 83.535   R^2: 0.164   \n",
      "Epoch: 23   Train Loss: 83.151   R^2: 0.168   \n",
      "Epoch: 24   Train Loss: 82.381   R^2: 0.169   \n",
      "Epoch: 25   Train Loss: 82.426   R^2: 0.172   \n",
      "Epoch: 26   Train Loss: 81.992   R^2: 0.177   \n",
      "Epoch: 27   Train Loss: 81.474   R^2: 0.183   \n",
      "Epoch: 28   Train Loss: 80.931   R^2: 0.187   \n",
      "Epoch: 29   Train Loss: 80.565   R^2: 0.191   \n",
      "Epoch: 30   Train Loss: 80.332   R^2: 0.193   \n",
      "Epoch: 31   Train Loss: 79.348   R^2: 0.199   \n",
      "Epoch: 32   Train Loss: 79.279   R^2: 0.207   \n",
      "Epoch: 33   Train Loss: 79.098   R^2: 0.211   \n",
      "Epoch: 34   Train Loss: 78.738   R^2: 0.203   \n",
      "Epoch: 35   Train Loss: 78.778   R^2: 0.205   \n",
      "Epoch: 36   Train Loss: 78.516   R^2: 0.212   \n",
      "Epoch: 37   Train Loss: 78.550   R^2: 0.215   \n",
      "Epoch: 38   Train Loss: 78.656   R^2: 0.210   \n",
      "Epoch: 39   Train Loss: 77.749   R^2: 0.214   \n",
      "Epoch: 40   Train Loss: 77.506   R^2: 0.220   \n",
      "Epoch: 41   Train Loss: 77.519   R^2: 0.209   \n",
      "Epoch: 42   Train Loss: 76.693   R^2: 0.211   \n",
      "Epoch: 43   Train Loss: 77.111   R^2: 0.222   \n",
      "Epoch: 44   Train Loss: 76.650   R^2: 0.232   \n",
      "Epoch: 45   Train Loss: 76.954   R^2: 0.219   \n",
      "Epoch: 46   Train Loss: 74.768   R^2: 0.242   \n",
      "Epoch: 47   Train Loss: 75.545   R^2: 0.246   \n",
      "Epoch: 48   Train Loss: 75.611   R^2: 0.245   \n",
      "Epoch: 49   Train Loss: 75.823   R^2: 0.236   \n",
      "Epoch: 50   Train Loss: 75.309   R^2: 0.249   \n",
      "Epoch: 51   Train Loss: 75.212   R^2: 0.242   \n",
      "Epoch: 52   Train Loss: 74.496   R^2: 0.241   \n",
      "Epoch: 53   Train Loss: 74.841   R^2: 0.242   \n",
      "Epoch: 54   Train Loss: 74.611   R^2: 0.242   \n",
      "Epoch: 55   Train Loss: 74.017   R^2: 0.254   \n",
      "Epoch: 56   Train Loss: 74.478   R^2: 0.256   \n",
      "Epoch: 57   Train Loss: 74.424   R^2: 0.250   \n",
      "Epoch: 58   Train Loss: 73.208   R^2: 0.268   \n",
      "Epoch: 59   Train Loss: 73.320   R^2: 0.254   \n",
      "Epoch: 60   Train Loss: 72.799   R^2: 0.272   \n",
      "Epoch: 61   Train Loss: 72.636   R^2: 0.266   \n",
      "Epoch: 62   Train Loss: 72.683   R^2: 0.265   \n",
      "Epoch: 63   Train Loss: 72.940   R^2: 0.259   \n",
      "Epoch: 64   Train Loss: 72.835   R^2: 0.270   \n",
      "Epoch: 65   Train Loss: 72.315   R^2: 0.269   \n",
      "Epoch: 66   Train Loss: 71.889   R^2: 0.278   \n",
      "Epoch: 67   Train Loss: 71.940   R^2: 0.274   \n",
      "Epoch: 68   Train Loss: 72.014   R^2: 0.277   \n",
      "Epoch: 69   Train Loss: 72.890   R^2: 0.272   \n",
      "Epoch: 70   Train Loss: 71.889   R^2: 0.273   \n",
      "Epoch: 71   Train Loss: 70.364   R^2: 0.291   \n",
      "Epoch: 72   Train Loss: 71.397   R^2: 0.279   \n",
      "Epoch: 73   Train Loss: 70.314   R^2: 0.290   \n",
      "Epoch: 74   Train Loss: 70.548   R^2: 0.282   \n",
      "Epoch: 75   Train Loss: 69.636   R^2: 0.296   \n",
      "Epoch: 76   Train Loss: 70.079   R^2: 0.287   \n",
      "Epoch: 77   Train Loss: 69.823   R^2: 0.301   \n",
      "Epoch: 78   Train Loss: 68.863   R^2: 0.301   \n",
      "Epoch: 79   Train Loss: 67.847   R^2: 0.314   \n",
      "Epoch: 80   Train Loss: 67.791   R^2: 0.305   \n",
      "Epoch: 81   Train Loss: 69.689   R^2: 0.296   \n",
      "Epoch: 82   Train Loss: 69.489   R^2: 0.297   \n",
      "Epoch: 83   Train Loss: 68.530   R^2: 0.305   \n",
      "Epoch: 84   Train Loss: 67.889   R^2: 0.312   \n",
      "Epoch: 85   Train Loss: 67.943   R^2: 0.309   \n",
      "Epoch: 86   Train Loss: 67.439   R^2: 0.308   \n",
      "Epoch: 87   Train Loss: 67.205   R^2: 0.315   \n",
      "Epoch: 88   Train Loss: 66.791   R^2: 0.316   \n",
      "Epoch: 89   Train Loss: 66.460   R^2: 0.328   \n",
      "Epoch: 90   Train Loss: 66.129   R^2: 0.337   \n",
      "Epoch: 91   Train Loss: 65.823   R^2: 0.333   \n",
      "Epoch: 92   Train Loss: 66.734   R^2: 0.321   \n",
      "Epoch: 93   Train Loss: 66.513   R^2: 0.326   \n",
      "Epoch: 94   Train Loss: 64.350   R^2: 0.345   \n",
      "Epoch: 95   Train Loss: 64.947   R^2: 0.342   \n",
      "Epoch: 96   Train Loss: 64.824   R^2: 0.332   \n",
      "Epoch: 97   Train Loss: 64.829   R^2: 0.350   \n",
      "Epoch: 98   Train Loss: 64.384   R^2: 0.339   \n",
      "Epoch: 99   Train Loss: 65.278   R^2: 0.330   \n",
      "Epoch: 100   Train Loss: 63.675   R^2: 0.341   \n",
      "Epoch: 101   Train Loss: 64.112   R^2: 0.350   \n",
      "Epoch: 102   Train Loss: 64.334   R^2: 0.347   \n",
      "Epoch: 103   Train Loss: 64.098   R^2: 0.354   \n",
      "Epoch: 104   Train Loss: 63.855   R^2: 0.351   \n",
      "Epoch: 105   Train Loss: 63.195   R^2: 0.350   \n",
      "Epoch: 106   Train Loss: 63.604   R^2: 0.344   \n",
      "Epoch: 107   Train Loss: 62.863   R^2: 0.356   \n",
      "Epoch: 108   Train Loss: 61.733   R^2: 0.371   \n",
      "Epoch: 109   Train Loss: 62.249   R^2: 0.344   \n",
      "Epoch: 110   Train Loss: 61.562   R^2: 0.363   \n",
      "Epoch: 111   Train Loss: 61.566   R^2: 0.370   \n",
      "Epoch: 112   Train Loss: 62.684   R^2: 0.366   \n",
      "Epoch: 113   Train Loss: 61.798   R^2: 0.365   \n",
      "Epoch: 114   Train Loss: 60.528   R^2: 0.380   \n",
      "Epoch: 115   Train Loss: 61.348   R^2: 0.373   \n",
      "Epoch: 116   Train Loss: 60.526   R^2: 0.382   \n",
      "Epoch: 117   Train Loss: 60.665   R^2: 0.380   \n",
      "Epoch: 118   Train Loss: 60.653   R^2: 0.370   \n",
      "Epoch: 119   Train Loss: 59.912   R^2: 0.385   \n",
      "Epoch: 120   Train Loss: 61.118   R^2: 0.370   \n",
      "Epoch: 121   Train Loss: 59.468   R^2: 0.382   \n",
      "Epoch: 122   Train Loss: 58.615   R^2: 0.393   \n",
      "Epoch: 123   Train Loss: 59.404   R^2: 0.378   \n",
      "Epoch: 124   Train Loss: 59.467   R^2: 0.393   \n",
      "Epoch: 125   Train Loss: 59.334   R^2: 0.370   \n",
      "Epoch: 126   Train Loss: 59.032   R^2: 0.398   \n",
      "Epoch: 127   Train Loss: 58.119   R^2: 0.402   \n",
      "Epoch: 128   Train Loss: 57.683   R^2: 0.407   \n",
      "Epoch: 129   Train Loss: 59.528   R^2: 0.386   \n",
      "Epoch: 130   Train Loss: 57.446   R^2: 0.401   \n",
      "Epoch: 131   Train Loss: 57.424   R^2: 0.414   \n",
      "Epoch: 132   Train Loss: 55.937   R^2: 0.416   \n",
      "Epoch: 133   Train Loss: 58.245   R^2: 0.392   \n",
      "Epoch: 134   Train Loss: 57.518   R^2: 0.410   \n",
      "Epoch: 135   Train Loss: 57.991   R^2: 0.398   \n",
      "Epoch: 136   Train Loss: 57.095   R^2: 0.406   \n",
      "Epoch: 137   Train Loss: 56.069   R^2: 0.423   \n",
      "Epoch: 138   Train Loss: 55.958   R^2: 0.421   \n",
      "Epoch: 139   Train Loss: 56.373   R^2: 0.426   \n",
      "Epoch: 140   Train Loss: 56.111   R^2: 0.411   \n",
      "Epoch: 141   Train Loss: 57.039   R^2: 0.406   \n",
      "Epoch: 142   Train Loss: 55.514   R^2: 0.423   \n",
      "Epoch: 143   Train Loss: 53.301   R^2: 0.450   \n",
      "Epoch: 144   Train Loss: 55.369   R^2: 0.431   \n",
      "Epoch: 145   Train Loss: 54.217   R^2: 0.442   \n",
      "Epoch: 146   Train Loss: 54.912   R^2: 0.420   \n",
      "Epoch: 147   Train Loss: 55.683   R^2: 0.431   \n",
      "Epoch: 148   Train Loss: 54.627   R^2: 0.423   \n",
      "Epoch: 149   Train Loss: 54.912   R^2: 0.442   \n",
      "Epoch: 150   Train Loss: 54.259   R^2: 0.433   \n",
      "Epoch: 151   Train Loss: 53.147   R^2: 0.448   \n",
      "Epoch: 152   Train Loss: 53.199   R^2: 0.444   \n",
      "Epoch: 153   Train Loss: 54.625   R^2: 0.424   \n",
      "Epoch: 154   Train Loss: 54.611   R^2: 0.443   \n",
      "Epoch: 155   Train Loss: 53.597   R^2: 0.447   \n",
      "Epoch: 156   Train Loss: 54.162   R^2: 0.430   \n",
      "Epoch: 157   Train Loss: 52.034   R^2: 0.444   \n",
      "Epoch: 158   Train Loss: 52.316   R^2: 0.457   \n",
      "Epoch: 159   Train Loss: 52.332   R^2: 0.457   \n",
      "Epoch: 160   Train Loss: 52.899   R^2: 0.450   \n",
      "Epoch: 161   Train Loss: 51.994   R^2: 0.467   \n",
      "Epoch: 162   Train Loss: 52.742   R^2: 0.449   \n",
      "Epoch: 163   Train Loss: 52.071   R^2: 0.451   \n",
      "Epoch: 164   Train Loss: 52.643   R^2: 0.448   \n",
      "Epoch: 165   Train Loss: 50.565   R^2: 0.472   \n",
      "Epoch: 166   Train Loss: 51.414   R^2: 0.466   \n",
      "Epoch: 167   Train Loss: 51.650   R^2: 0.462   \n",
      "Epoch: 168   Train Loss: 52.483   R^2: 0.452   \n",
      "Epoch: 169   Train Loss: 51.443   R^2: 0.472   \n",
      "Epoch: 170   Train Loss: 52.218   R^2: 0.466   \n",
      "Epoch: 171   Train Loss: 51.649   R^2: 0.462   \n",
      "Epoch: 172   Train Loss: 51.004   R^2: 0.471   \n",
      "Epoch: 173   Train Loss: 51.783   R^2: 0.459   \n",
      "Epoch: 174   Train Loss: 50.678   R^2: 0.465   \n",
      "Epoch: 175   Train Loss: 50.799   R^2: 0.455   \n",
      "Epoch: 176   Train Loss: 49.941   R^2: 0.481   \n",
      "Epoch: 177   Train Loss: 47.704   R^2: 0.513   \n",
      "Epoch: 178   Train Loss: 48.557   R^2: 0.490   \n",
      "Epoch: 179   Train Loss: 50.627   R^2: 0.471   \n",
      "Epoch: 180   Train Loss: 49.832   R^2: 0.471   \n",
      "Epoch: 181   Train Loss: 49.119   R^2: 0.480   \n",
      "Epoch: 182   Train Loss: 51.281   R^2: 0.470   \n",
      "Epoch: 183   Train Loss: 48.146   R^2: 0.495   \n",
      "Epoch: 184   Train Loss: 48.142   R^2: 0.485   \n",
      "Epoch: 185   Train Loss: 48.312   R^2: 0.499   \n",
      "Epoch: 186   Train Loss: 48.357   R^2: 0.486   \n",
      "Epoch: 187   Train Loss: 49.548   R^2: 0.474   \n",
      "Epoch: 188   Train Loss: 49.614   R^2: 0.471   \n",
      "Epoch: 189   Train Loss: 48.963   R^2: 0.478   \n",
      "Epoch: 190   Train Loss: 47.848   R^2: 0.492   \n",
      "Epoch: 191   Train Loss: 49.789   R^2: 0.480   \n",
      "Epoch: 192   Train Loss: 48.012   R^2: 0.497   \n",
      "Epoch: 193   Train Loss: 47.482   R^2: 0.495   \n",
      "Epoch: 194   Train Loss: 47.810   R^2: 0.494   \n",
      "Epoch: 195   Train Loss: 47.288   R^2: 0.506   \n",
      "Epoch: 196   Train Loss: 46.438   R^2: 0.517   \n",
      "Epoch: 197   Train Loss: 46.069   R^2: 0.512   \n",
      "Epoch: 198   Train Loss: 47.850   R^2: 0.488   \n",
      "Epoch: 199   Train Loss: 45.886   R^2: 0.519   \n",
      "Epoch: 200   Train Loss: 47.855   R^2: 0.479   \n",
      "Epoch: 201   Train Loss: 46.985   R^2: 0.501   \n",
      "Epoch: 202   Train Loss: 46.979   R^2: 0.507   \n",
      "Epoch: 203   Train Loss: 46.687   R^2: 0.494   \n",
      "Epoch: 204   Train Loss: 46.687   R^2: 0.520   \n",
      "Epoch: 205   Train Loss: 45.875   R^2: 0.511   \n",
      "Epoch: 206   Train Loss: 45.910   R^2: 0.511   \n",
      "Epoch: 207   Train Loss: 45.260   R^2: 0.524   \n",
      "Epoch: 208   Train Loss: 45.710   R^2: 0.516   \n",
      "Epoch: 209   Train Loss: 46.882   R^2: 0.504   \n",
      "Epoch: 210   Train Loss: 46.361   R^2: 0.518   \n",
      "Epoch: 211   Train Loss: 45.670   R^2: 0.519   \n",
      "Epoch: 212   Train Loss: 45.406   R^2: 0.527   \n",
      "Epoch: 213   Train Loss: 47.939   R^2: 0.485   \n",
      "Epoch: 214   Train Loss: 44.694   R^2: 0.528   \n",
      "Epoch: 215   Train Loss: 45.529   R^2: 0.516   \n",
      "Epoch: 216   Train Loss: 46.879   R^2: 0.526   \n",
      "Epoch: 217   Train Loss: 47.064   R^2: 0.503   \n",
      "Epoch: 218   Train Loss: 45.747   R^2: 0.528   \n",
      "Epoch: 219   Train Loss: 46.102   R^2: 0.509   \n",
      "Epoch: 220   Train Loss: 46.095   R^2: 0.512   \n",
      "Epoch: 221   Train Loss: 45.648   R^2: 0.529   \n",
      "Epoch: 222   Train Loss: 46.662   R^2: 0.501   \n",
      "Epoch: 223   Train Loss: 44.013   R^2: 0.532   \n",
      "Epoch: 224   Train Loss: 45.236   R^2: 0.519   \n",
      "Epoch: 225   Train Loss: 43.981   R^2: 0.541   \n",
      "Epoch: 226   Train Loss: 45.116   R^2: 0.536   \n",
      "Epoch: 227   Train Loss: 44.581   R^2: 0.530   \n",
      "Epoch: 228   Train Loss: 45.164   R^2: 0.525   \n",
      "Epoch: 229   Train Loss: 44.506   R^2: 0.526   \n",
      "Epoch: 230   Train Loss: 44.892   R^2: 0.532   \n",
      "Epoch: 231   Train Loss: 43.207   R^2: 0.554   \n",
      "Epoch: 232   Train Loss: 45.202   R^2: 0.535   \n",
      "Epoch: 233   Train Loss: 44.981   R^2: 0.529   \n",
      "Epoch: 234   Train Loss: 44.219   R^2: 0.531   \n",
      "Epoch: 235   Train Loss: 42.461   R^2: 0.551   \n",
      "Epoch: 236   Train Loss: 43.874   R^2: 0.537   \n",
      "Epoch: 237   Train Loss: 43.310   R^2: 0.541   \n",
      "Epoch: 238   Train Loss: 44.948   R^2: 0.514   \n",
      "Epoch: 239   Train Loss: 43.451   R^2: 0.542   \n",
      "Epoch: 240   Train Loss: 42.570   R^2: 0.554   \n",
      "Epoch: 241   Train Loss: 43.950   R^2: 0.536   \n",
      "Epoch: 242   Train Loss: 42.357   R^2: 0.551   \n",
      "Epoch: 243   Train Loss: 43.488   R^2: 0.539   \n",
      "Epoch: 244   Train Loss: 44.038   R^2: 0.533   \n",
      "Epoch: 245   Train Loss: 45.464   R^2: 0.518   \n",
      "Epoch: 246   Train Loss: 43.367   R^2: 0.538   \n",
      "Epoch: 247   Train Loss: 42.885   R^2: 0.542   \n",
      "Epoch: 248   Train Loss: 43.249   R^2: 0.535   \n",
      "Epoch: 249   Train Loss: 42.255   R^2: 0.556   \n",
      "Epoch: 250   Train Loss: 42.045   R^2: 0.540   \n",
      "Epoch: 251   Train Loss: 42.204   R^2: 0.550   \n",
      "Epoch: 252   Train Loss: 42.706   R^2: 0.539   \n",
      "Epoch: 253   Train Loss: 44.419   R^2: 0.531   \n",
      "Epoch: 254   Train Loss: 42.965   R^2: 0.548   \n",
      "Epoch: 255   Train Loss: 42.644   R^2: 0.550   \n",
      "Epoch: 256   Train Loss: 42.568   R^2: 0.547   \n",
      "Epoch: 257   Train Loss: 41.571   R^2: 0.564   \n",
      "Epoch: 258   Train Loss: 42.688   R^2: 0.549   \n",
      "Epoch: 259   Train Loss: 42.788   R^2: 0.552   \n",
      "Epoch: 260   Train Loss: 41.947   R^2: 0.554   \n",
      "Epoch: 261   Train Loss: 42.416   R^2: 0.549   \n",
      "Epoch: 262   Train Loss: 43.514   R^2: 0.544   \n",
      "Epoch: 263   Train Loss: 42.280   R^2: 0.552   \n",
      "Epoch: 264   Train Loss: 42.063   R^2: 0.551   \n",
      "Epoch: 265   Train Loss: 41.049   R^2: 0.562   \n",
      "Epoch: 266   Train Loss: 41.809   R^2: 0.544   \n",
      "Epoch: 267   Train Loss: 41.463   R^2: 0.566   \n",
      "Epoch: 268   Train Loss: 43.009   R^2: 0.545   \n",
      "Epoch: 269   Train Loss: 41.598   R^2: 0.559   \n",
      "Epoch: 270   Train Loss: 41.214   R^2: 0.562   \n",
      "Epoch: 271   Train Loss: 41.437   R^2: 0.562   \n",
      "Epoch: 272   Train Loss: 42.295   R^2: 0.551   \n",
      "Epoch: 273   Train Loss: 42.014   R^2: 0.558   \n",
      "Epoch: 274   Train Loss: 40.455   R^2: 0.571   \n",
      "Epoch: 275   Train Loss: 42.229   R^2: 0.557   \n",
      "Epoch: 276   Train Loss: 43.211   R^2: 0.538   \n",
      "Epoch: 277   Train Loss: 41.495   R^2: 0.561   \n",
      "Epoch: 278   Train Loss: 41.483   R^2: 0.560   \n",
      "Epoch: 279   Train Loss: 41.112   R^2: 0.567   \n",
      "Epoch: 280   Train Loss: 41.581   R^2: 0.566   \n",
      "Epoch: 281   Train Loss: 40.953   R^2: 0.549   \n",
      "Epoch: 282   Train Loss: 41.826   R^2: 0.558   \n",
      "Epoch: 283   Train Loss: 41.154   R^2: 0.557   \n",
      "Epoch: 284   Train Loss: 41.254   R^2: 0.558   \n",
      "Epoch: 285   Train Loss: 42.149   R^2: 0.545   \n",
      "Epoch: 286   Train Loss: 39.586   R^2: 0.588   \n",
      "Epoch: 287   Train Loss: 42.188   R^2: 0.566   \n",
      "Epoch: 288   Train Loss: 41.090   R^2: 0.566   \n",
      "Epoch: 289   Train Loss: 40.785   R^2: 0.571   \n",
      "Epoch: 290   Train Loss: 40.996   R^2: 0.576   \n",
      "Epoch: 291   Train Loss: 41.111   R^2: 0.567   \n",
      "Epoch: 292   Train Loss: 41.748   R^2: 0.554   \n",
      "Epoch: 293   Train Loss: 42.060   R^2: 0.548   \n",
      "Epoch: 294   Train Loss: 40.297   R^2: 0.571   \n",
      "Epoch: 295   Train Loss: 41.127   R^2: 0.572   \n",
      "Epoch: 296   Train Loss: 41.469   R^2: 0.569   \n",
      "Epoch: 297   Train Loss: 39.350   R^2: 0.585   \n",
      "Epoch: 298   Train Loss: 42.154   R^2: 0.551   \n",
      "Epoch: 299   Train Loss: 42.329   R^2: 0.521   \n",
      "Epoch: 300   Train Loss: 41.923   R^2: 0.562   \n",
      "Epoch: 301   Train Loss: 41.006   R^2: 0.568   \n",
      "Epoch: 302   Train Loss: 40.624   R^2: 0.568   \n",
      "Epoch: 303   Train Loss: 40.875   R^2: 0.559   \n",
      "Epoch: 304   Train Loss: 41.091   R^2: 0.560   \n",
      "Epoch: 305   Train Loss: 41.549   R^2: 0.559   \n",
      "Epoch: 306   Train Loss: 40.991   R^2: 0.555   \n",
      "Epoch: 307   Train Loss: 39.808   R^2: 0.576   \n",
      "Epoch: 308   Train Loss: 41.031   R^2: 0.582   \n",
      "Epoch: 309   Train Loss: 38.958   R^2: 0.588   \n",
      "Epoch: 310   Train Loss: 40.836   R^2: 0.563   \n",
      "Epoch: 311   Train Loss: 40.543   R^2: 0.570   \n",
      "Epoch: 312   Train Loss: 41.064   R^2: 0.567   \n",
      "Epoch: 313   Train Loss: 40.314   R^2: 0.577   \n",
      "Epoch: 314   Train Loss: 41.492   R^2: 0.560   \n",
      "Epoch: 315   Train Loss: 38.788   R^2: 0.584   \n",
      "Epoch: 316   Train Loss: 39.322   R^2: 0.585   \n",
      "Epoch: 317   Train Loss: 39.986   R^2: 0.590   \n",
      "Epoch: 318   Train Loss: 41.350   R^2: 0.564   \n",
      "Epoch: 319   Train Loss: 38.771   R^2: 0.597   \n",
      "Epoch: 320   Train Loss: 38.673   R^2: 0.595   \n",
      "Epoch: 321   Train Loss: 40.150   R^2: 0.577   \n",
      "Epoch: 322   Train Loss: 40.257   R^2: 0.578   \n",
      "Epoch: 323   Train Loss: 39.375   R^2: 0.583   \n",
      "Epoch: 324   Train Loss: 40.623   R^2: 0.574   \n",
      "Epoch: 325   Train Loss: 41.820   R^2: 0.549   \n",
      "Epoch: 326   Train Loss: 39.763   R^2: 0.585   \n",
      "Epoch: 327   Train Loss: 40.592   R^2: 0.567   \n",
      "Epoch: 328   Train Loss: 39.938   R^2: 0.576   \n",
      "Epoch: 329   Train Loss: 40.148   R^2: 0.572   \n",
      "Epoch: 330   Train Loss: 41.463   R^2: 0.553   \n",
      "Epoch: 331   Train Loss: 40.372   R^2: 0.570   \n",
      "Epoch: 332   Train Loss: 38.262   R^2: 0.593   \n",
      "Epoch: 333   Train Loss: 40.647   R^2: 0.569   \n",
      "Epoch: 334   Train Loss: 38.861   R^2: 0.579   \n",
      "Epoch: 335   Train Loss: 38.355   R^2: 0.581   \n",
      "Epoch: 336   Train Loss: 39.320   R^2: 0.581   \n",
      "Epoch: 337   Train Loss: 39.672   R^2: 0.579   \n",
      "Epoch: 338   Train Loss: 39.069   R^2: 0.584   \n",
      "Epoch: 339   Train Loss: 39.182   R^2: 0.593   \n",
      "Epoch: 340   Train Loss: 39.382   R^2: 0.575   \n",
      "Epoch: 341   Train Loss: 39.784   R^2: 0.571   \n",
      "Epoch: 342   Train Loss: 39.563   R^2: 0.587   \n",
      "Epoch: 343   Train Loss: 40.291   R^2: 0.580   \n",
      "Epoch: 344   Train Loss: 40.269   R^2: 0.578   \n",
      "Epoch: 345   Train Loss: 38.436   R^2: 0.587   \n",
      "Epoch: 346   Train Loss: 37.398   R^2: 0.604   \n",
      "Epoch: 347   Train Loss: 39.266   R^2: 0.588   \n",
      "Epoch: 348   Train Loss: 39.924   R^2: 0.575   \n",
      "Epoch: 349   Train Loss: 39.154   R^2: 0.583   \n",
      "Epoch: 350   Train Loss: 39.616   R^2: 0.580   \n",
      "training R2:  -50.82647430776044\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "num_epoch = epochs\n",
    "\n",
    "a_tr_loss = np.zeros([num_epoch])\n",
    "a_tr_Rsq = np.zeros([num_epoch])\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    #if epoch == 100:\n",
    "        #opt = optim.Adam(model.parameters(), lr=1e-6)\n",
    "    #if epoch == 100:\n",
    "        #opt = optim.Adam(model.parameters(), lr=8e-7)\n",
    "    #if epoch == 150:\n",
    "        #opt = optim.Adam(model.parameters(), lr=4e-7)\n",
    "    #if epoch == 200:\n",
    "        #opt = optim.Adam(model.parameters(), lr=2e-7)\n",
    "    #if epoch == 250:\n",
    "        #opt = optim.Adam(model.parameters(), lr=1e-7)\n",
    "    model.train() # put model in training mode\n",
    "    batch_loss_tr = []\n",
    "    batch_Rsq_tr = []\n",
    "    # iterate over training set\n",
    "    for train_iter, data in enumerate(train_loader):\n",
    "        x_batch,y_batch = data\n",
    "        \n",
    "        y_batch = y_batch.view(-1,1)\n",
    "        #y_batch = y_batch.type(torch.long)\n",
    "        \n",
    "        out = model(x_batch)\n",
    "        # Compute Loss\n",
    "        loss = criterion(out,y_batch.type(torch.float))\n",
    "        batch_loss_tr.append(loss.item())\n",
    "        # Compute R-square\n",
    "        Rsq = r2_score(y_batch.type(torch.float).detach().numpy(), out.detach().numpy())\n",
    "        batch_Rsq_tr.append(Rsq.item())\n",
    "        # Compute gradients using back propagation\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        # Take an optimization 'step'\n",
    "        opt.step()\n",
    "        \n",
    "    # Take scheduler step\n",
    "    scheduler.step()\n",
    "        \n",
    "    a_tr_loss[epoch] = np.mean(batch_loss_tr) # Compute average loss over epoch\n",
    "    a_tr_Rsq[epoch] = np.mean(batch_Rsq_tr)\n",
    "    print('Epoch: {0:2d}   Train Loss: {1:.3f}   '.format(epoch+1, a_tr_loss[epoch])\n",
    "         + 'R^2: {0:.3f}   '.format(a_tr_Rsq[epoch])\n",
    "         )\n",
    "    \n",
    "with torch.no_grad():\n",
    "    predict = model(torch.Tensor(Xtr_standardized)).detach().numpy().ravel()\n",
    "\n",
    "r2 = r2_score(ytr,predict)\n",
    "print('training R2: ',r2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "471376b7-d8f2-47e1-944e-89f48bad2861",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BassD\\anaconda3\\lib\\site-packages\\torch\\jit\\_trace.py:1084: TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error:\n",
      "Tensor-likes are not close!\n",
      "\n",
      "Mismatched elements: 1 / 1 (100.0%)\n",
      "Greatest absolute difference: 0.5616261959075928 at index (0, 0) (up to 1e-05 allowed)\n",
      "Greatest relative difference: 0.1533525974193798 at index (0, 0) (up to 1e-05 allowed)\n",
      "  _check_trace(\n"
     ]
    }
   ],
   "source": [
    "# save the model: you must use the .pth format for pytorch models!\n",
    "model_savepath = 'model.pth'\n",
    "\n",
    "# To save a PyTorch model, we first pass an input through the model, \n",
    "# and then save the \"trace\". \n",
    "# For this purpose, we can use any input. \n",
    "# We will create a random input with the proper dimension.\n",
    "x = torch.randn(26) # random input\n",
    "x = x[None,:] # add singleton batch index\n",
    "with torch.no_grad():\n",
    "    traced_cell = torch.jit.trace(model, (x))\n",
    "\n",
    "# Now we save the trace\n",
    "torch.jit.save(traced_cell, model_savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a961efc-47e6-4757-9bcb-3d590aa865e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training R2 =  0.6307979149648864\n",
      "test target predictions saved in yts_hat_pytorch.csv\n"
     ]
    }
   ],
   "source": [
    "# generate kaggle submission file using the validation script\n",
    "!python {\"validation.py \" + model_savepath + \" --Xts_path \" + Xts_savepath + \" --Xtr_path \" + Xtr_savepath + \" --yts_hat_path \" + yts_hat_savepath } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ba3c3a-aef0-441f-92dc-3c48da1b5937",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
