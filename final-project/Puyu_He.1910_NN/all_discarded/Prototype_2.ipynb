{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8b46637-09dd-4e48-846b-7a49ceb4af89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import r2_score\n",
    "from skorch import NeuralNetRegressor\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78dacaf1-6868-4ae5-befd-a1d4668bf8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "Xtr_loadpath = 'Xtr.csv'\n",
    "Xts_loadpath = 'Xts.csv'\n",
    "ytr_loadpath = 'ytr.csv'\n",
    "\n",
    "Xtr = np.loadtxt(Xtr_loadpath, delimiter=\",\")\n",
    "Xts = np.loadtxt(Xts_loadpath, delimiter=\",\")\n",
    "ytr = np.loadtxt(ytr_loadpath, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae45109c-c2ca-4718-869e-276b7140fdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize the training data\n",
    "Xtr_mean = np.mean(Xtr,axis=0)\n",
    "Xtr_std = np.std(Xtr,axis=0)\n",
    "ytr_mean = np.mean(ytr)\n",
    "ytr_std = np.std(ytr)\n",
    "\n",
    "Xtr_standardized = ((Xtr-Xtr_mean[None,:])/Xtr_std[None,:]) # revise this line as needed\n",
    "Xts_standardized = ((Xts-Xtr_mean[None,:])/Xtr_std[None,:]) # revise this line as needed\n",
    "ytr_standardized = ((ytr-ytr_mean)/ytr_std)\n",
    "\n",
    "# save the standardized training data\n",
    "Xtr_savepath = 'Xtr_pytorch.csv'\n",
    "Xts_savepath = 'Xts_pytorch.csv'\n",
    "ytr_savepath = 'ytr_pytorch.csv'\n",
    "yts_hat_savepath = 'yts_hat_pytorch.csv'\n",
    "\n",
    "np.savetxt(Xtr_savepath, Xtr_standardized, delimiter=\",\")\n",
    "np.savetxt(Xts_savepath, Xts_standardized, delimiter=\",\")\n",
    "np.savetxt(ytr_savepath, ytr_standardized, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f8624d1-9de3-4831-b7dd-577d5445b4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # feature selection\n",
    "\n",
    "# Xtr_feat_sel = Xtr_standardized\n",
    "# Xtr_feat_sel[:,1] = np.ones(Xtr_feat_sel.shape[0])*1e-5\n",
    "# Xtr_feat_sel[:,3] = np.ones(Xtr_feat_sel.shape[0])*1e-5\n",
    "# Xtr_feat_sel[:,4] = np.ones(Xtr_feat_sel.shape[0])*1e-5\n",
    "# Xtr_feat_sel[:,7] = np.ones(Xtr_feat_sel.shape[0])*1e-5\n",
    "# Xtr_feat_sel[:,10] = np.ones(Xtr_feat_sel.shape[0])*1e-5\n",
    "# Xtr_feat_sel[:,14] = np.ones(Xtr_feat_sel.shape[0])*1e-5\n",
    "# Xtr_feat_sel[:,15] = np.ones(Xtr_feat_sel.shape[0])*1e-5\n",
    "# Xtr_feat_sel[:,16] = np.ones(Xtr_feat_sel.shape[0])*1e-5\n",
    "# Xtr_feat_sel[:,17] = np.ones(Xtr_feat_sel.shape[0])*1e-5\n",
    "# Xtr_feat_sel[:,18] = np.ones(Xtr_feat_sel.shape[0])*1e-5\n",
    "# Xtr_feat_sel[:,19] = np.ones(Xtr_feat_sel.shape[0])*1e-5\n",
    "# Xtr_feat_sel[:,22] = np.ones(Xtr_feat_sel.shape[0])*1e-5\n",
    "# Xtr_feat_sel[:,23] = np.ones(Xtr_feat_sel.shape[0])*1e-5\n",
    "# Xtr_feat_sel[:,24] = np.ones(Xtr_feat_sel.shape[0])*1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22b1cbed-ee8a-4089-9372-b214cff5f85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the numpy arrays to PyTorch tensors\n",
    "Xtr_torch = torch.Tensor(Xtr_standardized)\n",
    "ytr_torch = torch.Tensor(ytr)\n",
    "\n",
    "batch_size = 100  # size of each batch\n",
    "\n",
    "# Create a training Dataset\n",
    "train_ds = torch.utils.data.TensorDataset(Xtr_torch, ytr_torch)\n",
    "# Creates a training DataLoader from this Dataset\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d678c59-51c2-4e42-bbf2-8cce993defda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a model\n",
    "# d_in = Xtr.shape[1]\n",
    "# d_h = 100\n",
    "# d_out = 1\n",
    "\n",
    "# class NeuralNet(nn.Module):\n",
    "#     def __init__(self,din,dh1,dh2,dh3,dout):\n",
    "#         super(NeuralNet, self).__init__()\n",
    "#         self.Dense1 = nn.Linear(din,dh1)\n",
    "#         self.Dense2 = nn.Linear(dh1,dh2)\n",
    "#         self.Dense3 = nn.Linear(dh2,dh3)\n",
    "#         self.Dense4 = nn.Linear(dh3,dout)\n",
    "#         self.ReLU = nn.ReLU()\n",
    "        \n",
    "#     def forward(self,x):\n",
    "#         x = self.ReLU(self.Dense1(x))\n",
    "#         x = self.ReLU(self.Dense2(x))        \n",
    "#         x = self.ReLU(self.Dense3(x))        \n",
    "#         out = self.Dense4(x)\n",
    "#         return out\n",
    "\n",
    "# model = NeuralNet(din=d_in, dh1=d_h, dh2=64*2, dh3=30*2, dout=d_out)\n",
    "\n",
    "# print(str(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a849753-8c0d-4901-9610-8127d3a23261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=26, out_features=2187, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Dropout(p=0.5, inplace=False)\n",
      "  (3): Linear(in_features=2187, out_features=729, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Dropout(p=0.5, inplace=False)\n",
      "  (6): Linear(in_features=729, out_features=243, bias=True)\n",
      "  (7): ReLU()\n",
      "  (8): Dropout(p=0.5, inplace=False)\n",
      "  (9): Linear(in_features=243, out_features=81, bias=True)\n",
      "  (10): ReLU()\n",
      "  (11): Linear(in_features=81, out_features=27, bias=True)\n",
      "  (12): ReLU()\n",
      "  (13): Linear(in_features=27, out_features=9, bias=True)\n",
      "  (14): ReLU()\n",
      "  (15): Linear(in_features=9, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "nin = Xtr.shape[1]\n",
    "nout = 1\n",
    "#nh = 256\n",
    "\n",
    "# model = nn.Sequential(\n",
    "#     nn.Linear(nin, 128*5),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Dropout(p=0.5),  # Add dropout layer with probability 0.2\n",
    "#     nn.Linear(128*5, 64*5),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Dropout(p=0.5),  # Add dropout layer with probability 0.2\n",
    "#     nn.Linear(64*5, 32*5),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Dropout(p=0.5),  # Add dropout layer with probability 0.2\n",
    "#     nn.Linear(32*5, 16*5),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Dropout(p=0.5),  # Add dropout layer with probability 0.2\n",
    "#     nn.Linear(16*5, 8*5),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(8*5, 4*5),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(4*5, 2*5),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(2*5, nout)\n",
    "# )\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(nin, 2187),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),  # Add dropout layer with probability 0.2\n",
    "    nn.Linear(2187, 729),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),  # Add dropout layer with probability 0.2\n",
    "    nn.Linear(729, 243),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),  # Add dropout layer with probability 0.2\n",
    "    nn.Linear(243, 81),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(81, 27),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(27, 9),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(9, nout)\n",
    ")\n",
    "\n",
    "print(str(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40f6e190-eefa-4e65-9755-9766c0ac95c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing the optimizer and loss function\n",
    "\n",
    "epochs = 350\n",
    "lrate = 2.5e-6\n",
    "decay = lrate/epochs\n",
    "lambda1 = lambda epoch: (1-decay)*epoch\n",
    "\n",
    "opt = optim.Adam(model.parameters(), lr=lrate)\n",
    "scheduler = optim.lr_scheduler.LambdaLR(opt, lr_lambda=lambda1)\n",
    "criterion = nn.MSELoss()\n",
    "#criterion = nn.HuberLoss(reduction='mean', delta=0.6)\n",
    "#criterion = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a53724c-0242-47c5-b0c1-7bfba9c13c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1   Train Loss: 101.007   R^2: -0.015   \n",
      "Epoch:  2   Train Loss: 101.007   R^2: -0.011   \n",
      "Epoch:  3   Train Loss: 101.001   R^2: -0.012   \n",
      "Epoch:  4   Train Loss: 100.992   R^2: -0.017   \n",
      "Epoch:  5   Train Loss: 100.965   R^2: -0.014   \n",
      "Epoch:  6   Train Loss: 100.884   R^2: -0.018   \n",
      "Epoch:  7   Train Loss: 100.617   R^2: -0.008   \n",
      "Epoch:  8   Train Loss: 99.512   R^2: 0.002   \n",
      "Epoch:  9   Train Loss: 96.717   R^2: 0.039   \n",
      "Epoch: 10   Train Loss: 94.701   R^2: 0.061   \n",
      "Epoch: 11   Train Loss: 93.469   R^2: 0.075   \n",
      "Epoch: 12   Train Loss: 91.929   R^2: 0.088   \n",
      "Epoch: 13   Train Loss: 90.254   R^2: 0.103   \n",
      "Epoch: 14   Train Loss: 88.693   R^2: 0.119   \n",
      "Epoch: 15   Train Loss: 87.273   R^2: 0.124   \n",
      "Epoch: 16   Train Loss: 86.560   R^2: 0.134   \n",
      "Epoch: 17   Train Loss: 85.690   R^2: 0.142   \n",
      "Epoch: 18   Train Loss: 84.291   R^2: 0.151   \n",
      "Epoch: 19   Train Loss: 83.318   R^2: 0.168   \n",
      "Epoch: 20   Train Loss: 82.272   R^2: 0.177   \n",
      "Epoch: 21   Train Loss: 81.663   R^2: 0.177   \n",
      "Epoch: 22   Train Loss: 80.595   R^2: 0.191   \n",
      "Epoch: 23   Train Loss: 79.807   R^2: 0.197   \n",
      "Epoch: 24   Train Loss: 79.126   R^2: 0.210   \n",
      "Epoch: 25   Train Loss: 78.831   R^2: 0.210   \n",
      "Epoch: 26   Train Loss: 77.956   R^2: 0.210   \n",
      "Epoch: 27   Train Loss: 78.514   R^2: 0.210   \n",
      "Epoch: 28   Train Loss: 77.845   R^2: 0.219   \n",
      "Epoch: 29   Train Loss: 76.521   R^2: 0.229   \n",
      "Epoch: 30   Train Loss: 75.473   R^2: 0.245   \n",
      "Epoch: 31   Train Loss: 75.575   R^2: 0.239   \n",
      "Epoch: 32   Train Loss: 75.487   R^2: 0.236   \n",
      "Epoch: 33   Train Loss: 75.180   R^2: 0.242   \n",
      "Epoch: 34   Train Loss: 75.601   R^2: 0.238   \n",
      "Epoch: 35   Train Loss: 73.798   R^2: 0.253   \n",
      "Epoch: 36   Train Loss: 73.678   R^2: 0.261   \n",
      "Epoch: 37   Train Loss: 73.206   R^2: 0.255   \n",
      "Epoch: 38   Train Loss: 73.675   R^2: 0.255   \n",
      "Epoch: 39   Train Loss: 73.038   R^2: 0.246   \n",
      "Epoch: 40   Train Loss: 72.652   R^2: 0.268   \n",
      "Epoch: 41   Train Loss: 73.391   R^2: 0.265   \n",
      "Epoch: 42   Train Loss: 71.847   R^2: 0.275   \n",
      "Epoch: 43   Train Loss: 71.408   R^2: 0.279   \n",
      "Epoch: 44   Train Loss: 72.326   R^2: 0.273   \n",
      "Epoch: 45   Train Loss: 70.038   R^2: 0.292   \n",
      "Epoch: 46   Train Loss: 71.478   R^2: 0.274   \n",
      "Epoch: 47   Train Loss: 71.000   R^2: 0.282   \n",
      "Epoch: 48   Train Loss: 70.367   R^2: 0.284   \n",
      "Epoch: 49   Train Loss: 70.080   R^2: 0.286   \n",
      "Epoch: 50   Train Loss: 69.441   R^2: 0.296   \n",
      "Epoch: 51   Train Loss: 69.993   R^2: 0.288   \n",
      "Epoch: 52   Train Loss: 69.197   R^2: 0.302   \n",
      "Epoch: 53   Train Loss: 69.169   R^2: 0.298   \n",
      "Epoch: 54   Train Loss: 68.401   R^2: 0.298   \n",
      "Epoch: 55   Train Loss: 68.503   R^2: 0.300   \n",
      "Epoch: 56   Train Loss: 67.968   R^2: 0.312   \n",
      "Epoch: 57   Train Loss: 68.085   R^2: 0.307   \n",
      "Epoch: 58   Train Loss: 67.224   R^2: 0.312   \n",
      "Epoch: 59   Train Loss: 66.878   R^2: 0.323   \n",
      "Epoch: 60   Train Loss: 66.726   R^2: 0.325   \n",
      "Epoch: 61   Train Loss: 67.342   R^2: 0.314   \n",
      "Epoch: 62   Train Loss: 65.373   R^2: 0.339   \n",
      "Epoch: 63   Train Loss: 66.275   R^2: 0.335   \n",
      "Epoch: 64   Train Loss: 65.113   R^2: 0.336   \n",
      "Epoch: 65   Train Loss: 65.717   R^2: 0.330   \n",
      "Epoch: 66   Train Loss: 64.964   R^2: 0.339   \n",
      "Epoch: 67   Train Loss: 63.446   R^2: 0.345   \n",
      "Epoch: 68   Train Loss: 63.315   R^2: 0.353   \n",
      "Epoch: 69   Train Loss: 65.504   R^2: 0.322   \n",
      "Epoch: 70   Train Loss: 63.986   R^2: 0.353   \n",
      "Epoch: 71   Train Loss: 62.883   R^2: 0.351   \n",
      "Epoch: 72   Train Loss: 63.712   R^2: 0.348   \n",
      "Epoch: 73   Train Loss: 62.913   R^2: 0.345   \n",
      "Epoch: 74   Train Loss: 63.218   R^2: 0.354   \n",
      "Epoch: 75   Train Loss: 62.658   R^2: 0.360   \n",
      "Epoch: 76   Train Loss: 60.964   R^2: 0.371   \n",
      "Epoch: 77   Train Loss: 62.570   R^2: 0.358   \n",
      "Epoch: 78   Train Loss: 61.285   R^2: 0.373   \n",
      "Epoch: 79   Train Loss: 60.977   R^2: 0.388   \n",
      "Epoch: 80   Train Loss: 60.083   R^2: 0.385   \n",
      "Epoch: 81   Train Loss: 60.253   R^2: 0.385   \n",
      "Epoch: 82   Train Loss: 59.244   R^2: 0.390   \n",
      "Epoch: 83   Train Loss: 60.452   R^2: 0.380   \n",
      "Epoch: 84   Train Loss: 60.179   R^2: 0.373   \n",
      "Epoch: 85   Train Loss: 60.492   R^2: 0.372   \n",
      "Epoch: 86   Train Loss: 59.191   R^2: 0.391   \n",
      "Epoch: 87   Train Loss: 58.994   R^2: 0.389   \n",
      "Epoch: 88   Train Loss: 58.073   R^2: 0.406   \n",
      "Epoch: 89   Train Loss: 59.588   R^2: 0.376   \n",
      "Epoch: 90   Train Loss: 59.014   R^2: 0.389   \n",
      "Epoch: 91   Train Loss: 58.797   R^2: 0.402   \n",
      "Epoch: 92   Train Loss: 57.483   R^2: 0.408   \n",
      "Epoch: 93   Train Loss: 57.619   R^2: 0.401   \n",
      "Epoch: 94   Train Loss: 55.964   R^2: 0.435   \n",
      "Epoch: 95   Train Loss: 56.908   R^2: 0.411   \n",
      "Epoch: 96   Train Loss: 56.968   R^2: 0.405   \n",
      "Epoch: 97   Train Loss: 56.419   R^2: 0.416   \n",
      "Epoch: 98   Train Loss: 55.689   R^2: 0.419   \n",
      "Epoch: 99   Train Loss: 56.232   R^2: 0.420   \n",
      "Epoch: 100   Train Loss: 55.087   R^2: 0.428   \n",
      "Epoch: 101   Train Loss: 55.155   R^2: 0.431   \n",
      "Epoch: 102   Train Loss: 54.162   R^2: 0.435   \n",
      "Epoch: 103   Train Loss: 54.517   R^2: 0.427   \n",
      "Epoch: 104   Train Loss: 53.026   R^2: 0.456   \n",
      "Epoch: 105   Train Loss: 53.860   R^2: 0.437   \n",
      "Epoch: 106   Train Loss: 54.041   R^2: 0.438   \n",
      "Epoch: 107   Train Loss: 55.061   R^2: 0.429   \n",
      "Epoch: 108   Train Loss: 53.599   R^2: 0.441   \n",
      "Epoch: 109   Train Loss: 52.228   R^2: 0.457   \n",
      "Epoch: 110   Train Loss: 52.101   R^2: 0.465   \n",
      "Epoch: 111   Train Loss: 50.925   R^2: 0.465   \n",
      "Epoch: 112   Train Loss: 52.581   R^2: 0.452   \n",
      "Epoch: 113   Train Loss: 50.574   R^2: 0.473   \n",
      "Epoch: 114   Train Loss: 51.771   R^2: 0.452   \n",
      "Epoch: 115   Train Loss: 52.096   R^2: 0.446   \n",
      "Epoch: 116   Train Loss: 51.885   R^2: 0.464   \n",
      "Epoch: 117   Train Loss: 50.371   R^2: 0.476   \n",
      "Epoch: 118   Train Loss: 48.702   R^2: 0.485   \n",
      "Epoch: 119   Train Loss: 50.646   R^2: 0.469   \n",
      "Epoch: 120   Train Loss: 50.004   R^2: 0.473   \n",
      "Epoch: 121   Train Loss: 50.339   R^2: 0.477   \n",
      "Epoch: 122   Train Loss: 50.897   R^2: 0.453   \n",
      "Epoch: 123   Train Loss: 48.998   R^2: 0.482   \n",
      "Epoch: 124   Train Loss: 49.631   R^2: 0.471   \n",
      "Epoch: 125   Train Loss: 48.114   R^2: 0.495   \n",
      "Epoch: 126   Train Loss: 49.433   R^2: 0.491   \n",
      "Epoch: 127   Train Loss: 50.520   R^2: 0.462   \n",
      "Epoch: 128   Train Loss: 48.358   R^2: 0.494   \n",
      "Epoch: 129   Train Loss: 48.210   R^2: 0.487   \n",
      "Epoch: 130   Train Loss: 48.386   R^2: 0.491   \n",
      "Epoch: 131   Train Loss: 47.999   R^2: 0.487   \n",
      "Epoch: 132   Train Loss: 47.594   R^2: 0.488   \n",
      "Epoch: 133   Train Loss: 46.870   R^2: 0.509   \n",
      "Epoch: 134   Train Loss: 47.185   R^2: 0.499   \n",
      "Epoch: 135   Train Loss: 46.084   R^2: 0.509   \n",
      "Epoch: 136   Train Loss: 46.155   R^2: 0.515   \n",
      "Epoch: 137   Train Loss: 46.364   R^2: 0.522   \n",
      "Epoch: 138   Train Loss: 45.331   R^2: 0.525   \n",
      "Epoch: 139   Train Loss: 46.779   R^2: 0.509   \n",
      "Epoch: 140   Train Loss: 46.203   R^2: 0.513   \n",
      "Epoch: 141   Train Loss: 47.344   R^2: 0.510   \n",
      "Epoch: 142   Train Loss: 44.223   R^2: 0.534   \n",
      "Epoch: 143   Train Loss: 46.333   R^2: 0.516   \n",
      "Epoch: 144   Train Loss: 44.220   R^2: 0.542   \n",
      "Epoch: 145   Train Loss: 44.536   R^2: 0.538   \n",
      "Epoch: 146   Train Loss: 44.155   R^2: 0.531   \n",
      "Epoch: 147   Train Loss: 46.314   R^2: 0.515   \n",
      "Epoch: 148   Train Loss: 44.258   R^2: 0.540   \n",
      "Epoch: 149   Train Loss: 43.575   R^2: 0.540   \n",
      "Epoch: 150   Train Loss: 44.329   R^2: 0.521   \n",
      "Epoch: 151   Train Loss: 43.094   R^2: 0.538   \n",
      "Epoch: 152   Train Loss: 44.427   R^2: 0.532   \n",
      "Epoch: 153   Train Loss: 44.331   R^2: 0.533   \n",
      "Epoch: 154   Train Loss: 43.350   R^2: 0.535   \n",
      "Epoch: 155   Train Loss: 42.834   R^2: 0.558   \n",
      "Epoch: 156   Train Loss: 42.045   R^2: 0.551   \n",
      "Epoch: 157   Train Loss: 43.068   R^2: 0.543   \n",
      "Epoch: 158   Train Loss: 43.113   R^2: 0.545   \n",
      "Epoch: 159   Train Loss: 42.683   R^2: 0.553   \n",
      "Epoch: 160   Train Loss: 40.974   R^2: 0.573   \n",
      "Epoch: 161   Train Loss: 40.942   R^2: 0.560   \n",
      "Epoch: 162   Train Loss: 42.002   R^2: 0.554   \n",
      "Epoch: 163   Train Loss: 41.719   R^2: 0.555   \n",
      "Epoch: 164   Train Loss: 42.859   R^2: 0.550   \n",
      "Epoch: 165   Train Loss: 41.650   R^2: 0.558   \n",
      "Epoch: 166   Train Loss: 40.881   R^2: 0.572   \n",
      "Epoch: 167   Train Loss: 41.860   R^2: 0.560   \n",
      "Epoch: 168   Train Loss: 41.587   R^2: 0.559   \n",
      "Epoch: 169   Train Loss: 41.444   R^2: 0.565   \n",
      "Epoch: 170   Train Loss: 40.269   R^2: 0.566   \n",
      "Epoch: 171   Train Loss: 40.507   R^2: 0.566   \n",
      "Epoch: 172   Train Loss: 40.401   R^2: 0.561   \n",
      "Epoch: 173   Train Loss: 39.694   R^2: 0.591   \n",
      "Epoch: 174   Train Loss: 39.472   R^2: 0.582   \n",
      "Epoch: 175   Train Loss: 40.039   R^2: 0.570   \n",
      "Epoch: 176   Train Loss: 40.009   R^2: 0.575   \n",
      "Epoch: 177   Train Loss: 39.629   R^2: 0.582   \n",
      "Epoch: 178   Train Loss: 40.056   R^2: 0.569   \n",
      "Epoch: 179   Train Loss: 38.914   R^2: 0.587   \n",
      "Epoch: 180   Train Loss: 37.943   R^2: 0.591   \n",
      "Epoch: 181   Train Loss: 40.078   R^2: 0.559   \n",
      "Epoch: 182   Train Loss: 39.943   R^2: 0.570   \n",
      "Epoch: 183   Train Loss: 37.221   R^2: 0.597   \n",
      "Epoch: 184   Train Loss: 38.843   R^2: 0.579   \n",
      "Epoch: 185   Train Loss: 38.288   R^2: 0.592   \n",
      "Epoch: 186   Train Loss: 37.878   R^2: 0.599   \n",
      "Epoch: 187   Train Loss: 39.891   R^2: 0.584   \n",
      "Epoch: 188   Train Loss: 38.489   R^2: 0.584   \n",
      "Epoch: 189   Train Loss: 37.988   R^2: 0.589   \n",
      "Epoch: 190   Train Loss: 40.114   R^2: 0.577   \n",
      "Epoch: 191   Train Loss: 37.672   R^2: 0.602   \n",
      "Epoch: 192   Train Loss: 39.319   R^2: 0.585   \n",
      "Epoch: 193   Train Loss: 38.318   R^2: 0.590   \n",
      "Epoch: 194   Train Loss: 38.809   R^2: 0.577   \n",
      "Epoch: 195   Train Loss: 38.990   R^2: 0.592   \n",
      "Epoch: 196   Train Loss: 39.447   R^2: 0.579   \n",
      "Epoch: 197   Train Loss: 38.359   R^2: 0.600   \n",
      "Epoch: 198   Train Loss: 38.039   R^2: 0.592   \n",
      "Epoch: 199   Train Loss: 37.658   R^2: 0.606   \n",
      "Epoch: 200   Train Loss: 37.724   R^2: 0.599   \n",
      "Epoch: 201   Train Loss: 36.335   R^2: 0.609   \n",
      "Epoch: 202   Train Loss: 36.341   R^2: 0.598   \n",
      "Epoch: 203   Train Loss: 36.767   R^2: 0.614   \n",
      "Epoch: 204   Train Loss: 36.760   R^2: 0.613   \n",
      "Epoch: 205   Train Loss: 37.102   R^2: 0.585   \n",
      "Epoch: 206   Train Loss: 37.789   R^2: 0.598   \n",
      "Epoch: 207   Train Loss: 36.498   R^2: 0.605   \n",
      "Epoch: 208   Train Loss: 38.921   R^2: 0.589   \n",
      "Epoch: 209   Train Loss: 37.890   R^2: 0.599   \n",
      "Epoch: 210   Train Loss: 35.758   R^2: 0.623   \n",
      "Epoch: 211   Train Loss: 35.946   R^2: 0.620   \n",
      "Epoch: 212   Train Loss: 35.314   R^2: 0.621   \n",
      "Epoch: 213   Train Loss: 36.287   R^2: 0.610   \n",
      "Epoch: 214   Train Loss: 36.184   R^2: 0.614   \n",
      "Epoch: 215   Train Loss: 37.012   R^2: 0.604   \n",
      "Epoch: 216   Train Loss: 36.713   R^2: 0.617   \n",
      "Epoch: 217   Train Loss: 37.557   R^2: 0.602   \n",
      "Epoch: 218   Train Loss: 35.859   R^2: 0.615   \n",
      "Epoch: 219   Train Loss: 35.441   R^2: 0.624   \n",
      "Epoch: 220   Train Loss: 35.834   R^2: 0.613   \n",
      "Epoch: 221   Train Loss: 35.802   R^2: 0.624   \n",
      "Epoch: 222   Train Loss: 34.422   R^2: 0.633   \n",
      "Epoch: 223   Train Loss: 35.033   R^2: 0.627   \n",
      "Epoch: 224   Train Loss: 36.249   R^2: 0.606   \n",
      "Epoch: 225   Train Loss: 36.190   R^2: 0.621   \n",
      "Epoch: 226   Train Loss: 36.520   R^2: 0.600   \n",
      "Epoch: 227   Train Loss: 35.108   R^2: 0.619   \n",
      "Epoch: 228   Train Loss: 36.408   R^2: 0.616   \n",
      "Epoch: 229   Train Loss: 34.491   R^2: 0.624   \n",
      "Epoch: 230   Train Loss: 35.372   R^2: 0.629   \n",
      "Epoch: 231   Train Loss: 34.469   R^2: 0.637   \n",
      "Epoch: 232   Train Loss: 35.947   R^2: 0.620   \n",
      "Epoch: 233   Train Loss: 36.209   R^2: 0.616   \n",
      "Epoch: 234   Train Loss: 35.905   R^2: 0.615   \n",
      "Epoch: 235   Train Loss: 35.980   R^2: 0.595   \n",
      "Epoch: 236   Train Loss: 35.090   R^2: 0.621   \n",
      "Epoch: 237   Train Loss: 35.164   R^2: 0.623   \n",
      "Epoch: 238   Train Loss: 34.953   R^2: 0.630   \n",
      "Epoch: 239   Train Loss: 35.072   R^2: 0.626   \n",
      "Epoch: 240   Train Loss: 35.006   R^2: 0.625   \n",
      "Epoch: 241   Train Loss: 33.391   R^2: 0.638   \n",
      "Epoch: 242   Train Loss: 33.450   R^2: 0.640   \n",
      "Epoch: 243   Train Loss: 34.332   R^2: 0.638   \n",
      "Epoch: 244   Train Loss: 35.430   R^2: 0.621   \n",
      "Epoch: 245   Train Loss: 34.030   R^2: 0.624   \n",
      "Epoch: 246   Train Loss: 34.100   R^2: 0.632   \n",
      "Epoch: 247   Train Loss: 34.526   R^2: 0.638   \n",
      "Epoch: 248   Train Loss: 34.470   R^2: 0.628   \n",
      "Epoch: 249   Train Loss: 33.540   R^2: 0.640   \n",
      "Epoch: 250   Train Loss: 35.504   R^2: 0.621   \n",
      "Epoch: 251   Train Loss: 33.880   R^2: 0.633   \n",
      "Epoch: 252   Train Loss: 33.728   R^2: 0.644   \n",
      "Epoch: 253   Train Loss: 32.174   R^2: 0.658   \n",
      "Epoch: 254   Train Loss: 32.506   R^2: 0.656   \n",
      "Epoch: 255   Train Loss: 34.396   R^2: 0.636   \n",
      "Epoch: 256   Train Loss: 36.320   R^2: 0.623   \n",
      "Epoch: 257   Train Loss: 34.889   R^2: 0.639   \n",
      "Epoch: 258   Train Loss: 34.379   R^2: 0.628   \n",
      "Epoch: 259   Train Loss: 34.499   R^2: 0.629   \n",
      "Epoch: 260   Train Loss: 33.728   R^2: 0.638   \n",
      "Epoch: 261   Train Loss: 34.073   R^2: 0.642   \n",
      "Epoch: 262   Train Loss: 33.864   R^2: 0.644   \n",
      "Epoch: 263   Train Loss: 32.327   R^2: 0.658   \n",
      "Epoch: 264   Train Loss: 32.334   R^2: 0.649   \n",
      "Epoch: 265   Train Loss: 34.072   R^2: 0.639   \n",
      "Epoch: 266   Train Loss: 32.621   R^2: 0.649   \n",
      "Epoch: 267   Train Loss: 33.527   R^2: 0.644   \n",
      "Epoch: 268   Train Loss: 34.132   R^2: 0.640   \n",
      "Epoch: 269   Train Loss: 32.561   R^2: 0.655   \n",
      "Epoch: 270   Train Loss: 33.466   R^2: 0.648   \n",
      "Epoch: 271   Train Loss: 33.079   R^2: 0.656   \n",
      "Epoch: 272   Train Loss: 33.778   R^2: 0.634   \n",
      "Epoch: 273   Train Loss: 31.952   R^2: 0.654   \n",
      "Epoch: 274   Train Loss: 32.642   R^2: 0.659   \n",
      "Epoch: 275   Train Loss: 34.778   R^2: 0.644   \n",
      "Epoch: 276   Train Loss: 34.837   R^2: 0.630   \n",
      "Epoch: 277   Train Loss: 32.261   R^2: 0.655   \n",
      "Epoch: 278   Train Loss: 33.199   R^2: 0.643   \n",
      "Epoch: 279   Train Loss: 35.117   R^2: 0.629   \n",
      "Epoch: 280   Train Loss: 34.126   R^2: 0.629   \n",
      "Epoch: 281   Train Loss: 33.864   R^2: 0.621   \n",
      "Epoch: 282   Train Loss: 33.015   R^2: 0.657   \n",
      "Epoch: 283   Train Loss: 33.645   R^2: 0.640   \n",
      "Epoch: 284   Train Loss: 31.710   R^2: 0.665   \n",
      "Epoch: 285   Train Loss: 32.063   R^2: 0.652   \n",
      "Epoch: 286   Train Loss: 33.713   R^2: 0.651   \n",
      "Epoch: 287   Train Loss: 32.691   R^2: 0.646   \n",
      "Epoch: 288   Train Loss: 32.504   R^2: 0.654   \n",
      "Epoch: 289   Train Loss: 32.041   R^2: 0.655   \n",
      "Epoch: 290   Train Loss: 32.037   R^2: 0.662   \n",
      "Epoch: 291   Train Loss: 33.640   R^2: 0.647   \n",
      "Epoch: 292   Train Loss: 32.932   R^2: 0.652   \n",
      "Epoch: 293   Train Loss: 33.226   R^2: 0.644   \n",
      "Epoch: 294   Train Loss: 32.290   R^2: 0.663   \n",
      "Epoch: 295   Train Loss: 33.432   R^2: 0.642   \n",
      "Epoch: 296   Train Loss: 33.465   R^2: 0.651   \n",
      "Epoch: 297   Train Loss: 32.458   R^2: 0.653   \n",
      "Epoch: 298   Train Loss: 32.396   R^2: 0.655   \n",
      "Epoch: 299   Train Loss: 33.421   R^2: 0.643   \n",
      "Epoch: 300   Train Loss: 33.127   R^2: 0.653   \n",
      "Epoch: 301   Train Loss: 33.362   R^2: 0.653   \n",
      "Epoch: 302   Train Loss: 33.340   R^2: 0.650   \n",
      "Epoch: 303   Train Loss: 33.975   R^2: 0.642   \n",
      "Epoch: 304   Train Loss: 32.346   R^2: 0.650   \n",
      "Epoch: 305   Train Loss: 30.282   R^2: 0.679   \n",
      "Epoch: 306   Train Loss: 30.051   R^2: 0.679   \n",
      "Epoch: 307   Train Loss: 31.353   R^2: 0.669   \n",
      "Epoch: 308   Train Loss: 33.042   R^2: 0.639   \n",
      "Epoch: 309   Train Loss: 32.770   R^2: 0.649   \n",
      "Epoch: 310   Train Loss: 32.704   R^2: 0.658   \n",
      "Epoch: 311   Train Loss: 32.388   R^2: 0.657   \n",
      "Epoch: 312   Train Loss: 32.974   R^2: 0.654   \n",
      "Epoch: 313   Train Loss: 32.058   R^2: 0.659   \n",
      "Epoch: 314   Train Loss: 32.716   R^2: 0.648   \n",
      "Epoch: 315   Train Loss: 33.143   R^2: 0.651   \n",
      "Epoch: 316   Train Loss: 33.727   R^2: 0.639   \n",
      "Epoch: 317   Train Loss: 30.631   R^2: 0.677   \n",
      "Epoch: 318   Train Loss: 32.992   R^2: 0.662   \n",
      "Epoch: 319   Train Loss: 32.791   R^2: 0.643   \n",
      "Epoch: 320   Train Loss: 31.733   R^2: 0.659   \n",
      "Epoch: 321   Train Loss: 32.178   R^2: 0.657   \n",
      "Epoch: 322   Train Loss: 32.775   R^2: 0.650   \n",
      "Epoch: 323   Train Loss: 32.570   R^2: 0.643   \n",
      "Epoch: 324   Train Loss: 31.348   R^2: 0.673   \n",
      "Epoch: 325   Train Loss: 31.596   R^2: 0.656   \n",
      "Epoch: 326   Train Loss: 33.618   R^2: 0.639   \n",
      "Epoch: 327   Train Loss: 32.276   R^2: 0.627   \n",
      "Epoch: 328   Train Loss: 32.713   R^2: 0.656   \n",
      "Epoch: 329   Train Loss: 31.843   R^2: 0.666   \n",
      "Epoch: 330   Train Loss: 32.397   R^2: 0.650   \n",
      "Epoch: 331   Train Loss: 30.255   R^2: 0.676   \n",
      "Epoch: 332   Train Loss: 32.388   R^2: 0.655   \n",
      "Epoch: 333   Train Loss: 31.619   R^2: 0.655   \n",
      "Epoch: 334   Train Loss: 32.283   R^2: 0.655   \n",
      "Epoch: 335   Train Loss: 33.578   R^2: 0.642   \n",
      "Epoch: 336   Train Loss: 31.754   R^2: 0.647   \n",
      "Epoch: 337   Train Loss: 32.436   R^2: 0.657   \n",
      "Epoch: 338   Train Loss: 31.435   R^2: 0.669   \n",
      "Epoch: 339   Train Loss: 31.588   R^2: 0.655   \n",
      "Epoch: 340   Train Loss: 30.884   R^2: 0.671   \n",
      "Epoch: 341   Train Loss: 30.670   R^2: 0.674   \n",
      "Epoch: 342   Train Loss: 32.266   R^2: 0.654   \n",
      "Epoch: 343   Train Loss: 32.166   R^2: 0.658   \n",
      "Epoch: 344   Train Loss: 31.506   R^2: 0.660   \n",
      "Epoch: 345   Train Loss: 30.700   R^2: 0.679   \n",
      "Epoch: 346   Train Loss: 32.142   R^2: 0.651   \n",
      "Epoch: 347   Train Loss: 31.909   R^2: 0.665   \n",
      "Epoch: 348   Train Loss: 31.563   R^2: 0.657   \n",
      "Epoch: 349   Train Loss: 31.536   R^2: 0.662   \n",
      "Epoch: 350   Train Loss: 31.458   R^2: 0.668   \n",
      "training R2:  0.7069303569042282\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "num_epoch = epochs\n",
    "\n",
    "a_tr_loss = np.zeros([num_epoch])\n",
    "a_tr_Rsq = np.zeros([num_epoch])\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    #if epoch == 100:\n",
    "        #opt = optim.Adam(model.parameters(), lr=1e-6)\n",
    "    #if epoch == 100:\n",
    "        #opt = optim.Adam(model.parameters(), lr=8e-7)\n",
    "    #if epoch == 150:\n",
    "        #opt = optim.Adam(model.parameters(), lr=4e-7)\n",
    "    #if epoch == 200:\n",
    "        #opt = optim.Adam(model.parameters(), lr=2e-7)\n",
    "    #if epoch == 250:\n",
    "        #opt = optim.Adam(model.parameters(), lr=1e-7)\n",
    "    model.train() # put model in training mode\n",
    "    batch_loss_tr = []\n",
    "    batch_Rsq_tr = []\n",
    "    # iterate over training set\n",
    "    for train_iter, data in enumerate(train_loader):\n",
    "        x_batch,y_batch = data\n",
    "        \n",
    "        y_batch = y_batch.view(-1,1)\n",
    "        #y_batch = y_batch.type(torch.long)\n",
    "        \n",
    "        out = model(x_batch)\n",
    "        # Compute Loss\n",
    "        loss = criterion(out,y_batch.type(torch.float))\n",
    "        batch_loss_tr.append(loss.item())\n",
    "        # Compute R-square\n",
    "        Rsq = r2_score(y_batch.type(torch.float).detach().numpy(), out.detach().numpy())\n",
    "        batch_Rsq_tr.append(Rsq.item())\n",
    "        # Compute gradients using back propagation\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        # Take an optimization 'step'\n",
    "        opt.step()\n",
    "        \n",
    "    # Take scheduler step\n",
    "    scheduler.step()\n",
    "        \n",
    "    a_tr_loss[epoch] = np.mean(batch_loss_tr) # Compute average loss over epoch\n",
    "    a_tr_Rsq[epoch] = np.mean(batch_Rsq_tr)\n",
    "    print('Epoch: {0:2d}   Train Loss: {1:.3f}   '.format(epoch+1, a_tr_loss[epoch])\n",
    "         + 'R^2: {0:.3f}   '.format(a_tr_Rsq[epoch])\n",
    "         )\n",
    "    \n",
    "with torch.no_grad():\n",
    "    predict = model(torch.Tensor(Xtr_standardized)).detach().numpy().ravel()\n",
    "\n",
    "r2 = r2_score(ytr,predict)\n",
    "print('training R2: ',r2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "471376b7-d8f2-47e1-944e-89f48bad2861",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BassD\\anaconda3\\lib\\site-packages\\torch\\jit\\_trace.py:1084: TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error:\n",
      "Tensor-likes are not close!\n",
      "\n",
      "Mismatched elements: 1 / 1 (100.0%)\n",
      "Greatest absolute difference: 1.7879984378814697 at index (0, 0) (up to 1e-05 allowed)\n",
      "Greatest relative difference: 1.2870993137160507 at index (0, 0) (up to 1e-05 allowed)\n",
      "  _check_trace(\n"
     ]
    }
   ],
   "source": [
    "# save the model: you must use the .pth format for pytorch models!\n",
    "model_savepath = 'model.pth'\n",
    "\n",
    "# To save a PyTorch model, we first pass an input through the model, \n",
    "# and then save the \"trace\". \n",
    "# For this purpose, we can use any input. \n",
    "# We will create a random input with the proper dimension.\n",
    "x = torch.randn(26) # random input\n",
    "x = x[None,:] # add singleton batch index\n",
    "with torch.no_grad():\n",
    "    traced_cell = torch.jit.trace(model, (x))\n",
    "\n",
    "# Now we save the trace\n",
    "torch.jit.save(traced_cell, model_savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a961efc-47e6-4757-9bcb-3d590aa865e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training R2 =  0.718322265149214\n",
      "test target predictions saved in yts_hat_pytorch.csv\n"
     ]
    }
   ],
   "source": [
    "# generate kaggle submission file using the validation script\n",
    "!python {\"validation.py \" + model_savepath + \" --Xts_path \" + Xts_savepath + \" --Xtr_path \" + Xtr_savepath + \" --yts_hat_path \" + yts_hat_savepath } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ba3c3a-aef0-441f-92dc-3c48da1b5937",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
