{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8b46637-09dd-4e48-846b-7a49ceb4af89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import r2_score\n",
    "from skorch import NeuralNetRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78dacaf1-6868-4ae5-befd-a1d4668bf8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "Xtr_loadpath = 'Xtr.csv'\n",
    "Xts_loadpath = 'Xts.csv'\n",
    "ytr_loadpath = 'ytr.csv'\n",
    "\n",
    "Xtr = np.loadtxt(Xtr_loadpath, delimiter=\",\")\n",
    "Xts = np.loadtxt(Xts_loadpath, delimiter=\",\")\n",
    "ytr = np.loadtxt(ytr_loadpath, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae45109c-c2ca-4718-869e-276b7140fdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize the training data\n",
    "Xtr_mean = np.mean(Xtr,axis=0)\n",
    "Xtr_std = np.std(Xtr,axis=0)\n",
    "ytr_mean = np.mean(ytr)\n",
    "ytr_std = np.std(ytr)\n",
    "\n",
    "Xtr_standardized = ((Xtr-Xtr_mean[None,:])/Xtr_std[None,:]) # revise this line as needed\n",
    "Xts_standardized = ((Xts-Xtr_mean[None,:])/Xtr_std[None,:]) # revise this line as needed\n",
    "ytr_standardized = ((ytr-ytr_mean)/ytr_std)\n",
    "\n",
    "# save the standardized training data\n",
    "Xtr_savepath = 'Xtr_pytorch.csv'\n",
    "Xts_savepath = 'Xts_pytorch.csv'\n",
    "ytr_savepath = 'ytr_pytorch.csv'\n",
    "yts_hat_savepath = 'yts_hat_pytorch.csv'\n",
    "\n",
    "np.savetxt(Xtr_savepath, Xtr_standardized, delimiter=\",\")\n",
    "np.savetxt(Xts_savepath, Xts_standardized, delimiter=\",\")\n",
    "np.savetxt(ytr_savepath, ytr_standardized, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22b1cbed-ee8a-4089-9372-b214cff5f85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the numpy arrays to PyTorch tensors\n",
    "Xtr_torch = torch.Tensor(Xtr_standardized)\n",
    "ytr_torch = torch.Tensor(ytr)\n",
    "\n",
    "batch_size = 100  # size of each batch\n",
    "\n",
    "# Create a training Dataset\n",
    "train_ds = torch.utils.data.TensorDataset(Xtr_torch, ytr_torch)\n",
    "# Creates a training DataLoader from this Dataset\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a849753-8c0d-4901-9610-8127d3a23261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=26, out_features=4879, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Dropout(p=0.5, inplace=False)\n",
      "  (3): Linear(in_features=4879, out_features=2187, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Dropout(p=0.5, inplace=False)\n",
      "  (6): Linear(in_features=2187, out_features=1262, bias=True)\n",
      "  (7): ReLU()\n",
      "  (8): Dropout(p=0.5, inplace=False)\n",
      "  (9): Linear(in_features=1262, out_features=729, bias=True)\n",
      "  (10): ReLU()\n",
      "  (11): Dropout(p=0.5, inplace=False)\n",
      "  (12): Linear(in_features=729, out_features=420, bias=True)\n",
      "  (13): ReLU()\n",
      "  (14): Dropout(p=0.5, inplace=False)\n",
      "  (15): Linear(in_features=420, out_features=243, bias=True)\n",
      "  (16): ReLU()\n",
      "  (17): Dropout(p=0.5, inplace=False)\n",
      "  (18): Linear(in_features=243, out_features=81, bias=True)\n",
      "  (19): ReLU()\n",
      "  (20): Dropout(p=0.5, inplace=False)\n",
      "  (21): Linear(in_features=81, out_features=27, bias=True)\n",
      "  (22): ReLU()\n",
      "  (23): Linear(in_features=27, out_features=9, bias=True)\n",
      "  (24): ReLU()\n",
      "  (25): Linear(in_features=9, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "nin = Xtr.shape[1]\n",
    "nout = 1\n",
    "\n",
    "# model = nn.Sequential(\n",
    "#     nn.Linear(nin, 128*5),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Dropout(p=0.5),  # Add dropout layer with probability 0.2\n",
    "#     nn.Linear(128*5, 64*5),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Dropout(p=0.5),  # Add dropout layer with probability 0.2\n",
    "#     nn.Linear(64*5, 32*5),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Dropout(p=0.5),  # Add dropout layer with probability 0.2\n",
    "#     nn.Linear(32*5, 16*5),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Dropout(p=0.5),  # Add dropout layer with probability 0.2\n",
    "#     nn.Linear(16*5, 8*5),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(8*5, 4*5),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(4*5, 2*5),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(2*5, nout)\n",
    "# )\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(nin, 4879),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),  # Add dropout layer with probability 0.2\n",
    "    nn.Linear(4879, 2187),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),  # Add dropout layer with probability 0.2\n",
    "    nn.Linear(2187, 1262),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),  # Add dropout layer with probability 0.2\n",
    "    nn.Linear(1262, 729),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),  # Add dropout layer with probability 0.2\n",
    "    nn.Linear(729, 420),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),  # Add dropout layer with probability 0.2   \n",
    "    nn.Linear(420, 243),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),  # Add dropout layer with probability 0.2  \n",
    "    nn.Linear(243, 81),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),  # Add dropout layer with probability 0.2\n",
    "    nn.Linear(81, 27),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(27, 9),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(9, nout)\n",
    ")\n",
    "\n",
    "print(str(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40f6e190-eefa-4e65-9755-9766c0ac95c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing the optimizer and loss function\n",
    "\n",
    "epochs = 700\n",
    "lrate = 1e-4\n",
    "decay = lrate/epochs\n",
    "lambda1 = lambda epoch: (1-decay)*epoch\n",
    "\n",
    "opt = optim.Adam(model.parameters(), lr=lrate)\n",
    "scheduler = optim.lr_scheduler.LambdaLR(opt, lr_lambda=lambda1)\n",
    "criterion = nn.MSELoss()\n",
    "#criterion = nn.HuberLoss(reduction='mean', delta=30)\n",
    "#criterion = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a53724c-0242-47c5-b0c1-7bfba9c13c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10   Train Loss: 78.588   R^2: 0.210   \n",
      "Epoch: 20   Train Loss: 76.415   R^2: 0.220   \n",
      "Epoch: 30   Train Loss: 78.447   R^2: 0.213   \n",
      "Epoch: 40   Train Loss: 85.164   R^2: 0.154   \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28496\\806416831.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;31m# Take an optimization 'step'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;31m# Take scheduler step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m             \u001b[1;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    278\u001b[0m                                                f\"but got {result}.\")\n\u001b[0;32m    279\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 280\u001b[1;33m                 \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    281\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'differentiable'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprev_grad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    139\u001b[0m                 state_steps)\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             adam(\n\u001b[0m\u001b[0;32m    142\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    279\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 281\u001b[1;33m     func(params,\n\u001b[0m\u001b[0;32m    282\u001b[0m          \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    283\u001b[0m          \u001b[0mexp_avgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    391\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 393\u001b[1;33m             \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "num_epoch = epochs\n",
    "\n",
    "a_tr_loss = np.zeros([num_epoch])\n",
    "a_tr_Rsq = np.zeros([num_epoch])\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "\n",
    "    model.train() # put model in training mode\n",
    "    batch_loss_tr = []\n",
    "    batch_Rsq_tr = []\n",
    "    # iterate over training set\n",
    "    for train_iter, data in enumerate(train_loader):\n",
    "        x_batch,y_batch = data\n",
    "        \n",
    "        y_batch = y_batch.view(-1,1)\n",
    "        #y_batch = y_batch.type(torch.long)\n",
    "        \n",
    "        out = model(x_batch)\n",
    "        # Compute Loss\n",
    "        loss = criterion(out,y_batch.type(torch.float))\n",
    "        batch_loss_tr.append(loss.item())\n",
    "        # Compute R-square\n",
    "        Rsq = r2_score(y_batch.type(torch.float).detach().numpy(), out.detach().numpy())\n",
    "        batch_Rsq_tr.append(Rsq.item())\n",
    "        # Compute gradients using back propagation\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        # Take an optimization 'step'\n",
    "        opt.step()\n",
    "        \n",
    "    # Take scheduler step\n",
    "    scheduler.step()\n",
    "        \n",
    "    a_tr_loss[epoch] = np.mean(batch_loss_tr) # Compute average loss over epoch\n",
    "    a_tr_Rsq[epoch] = np.mean(batch_Rsq_tr)\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print('Epoch: {0:2d}   Train Loss: {1:.3f}   '.format(epoch+1, a_tr_loss[epoch])\n",
    "             + 'R^2: {0:.3f}   '.format(a_tr_Rsq[epoch])\n",
    "             )\n",
    "    \n",
    "with torch.no_grad():\n",
    "    predict = model(torch.Tensor(Xtr_standardized)).detach().numpy().ravel()\n",
    "\n",
    "r2 = r2_score(ytr,predict)\n",
    "print('training R2: ',r2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286cf170-2380-4782-8ebe-a4d8d925b8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nepochs = len(a_tr_Rsq)\n",
    "plt.plot(np.arange(1,nepochs+1), a_tr_Rsq, 'o-', linewidth=1)\n",
    "plt.grid()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Train accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318343c0-4316-466a-b376-c8ecf89b262c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsteps = len(a_tr_loss)\n",
    "ntr = Xtr.shape[0]\n",
    "epochs = np.arange(1,nsteps+1)*batch_size/ntr\n",
    "plt.semilogy(epochs, a_tr_loss)\n",
    "plt.xlabel('Epochs', fontsize=16)\n",
    "plt.ylabel('Loss', fontsize=16)\n",
    "plt.grid()\n",
    "plt.xlim([0,np.max(epochs)])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471376b7-d8f2-47e1-944e-89f48bad2861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model: you must use the .pth format for pytorch models!\n",
    "model_savepath = 'model.pth'\n",
    "\n",
    "# To save a PyTorch model, we first pass an input through the model, \n",
    "# and then save the \"trace\". \n",
    "# For this purpose, we can use any input. \n",
    "# We will create a random input with the proper dimension.\n",
    "x = torch.randn(26) # random input\n",
    "x = x[None,:] # add singleton batch index\n",
    "with torch.no_grad():\n",
    "    traced_cell = torch.jit.trace(model, (x))\n",
    "\n",
    "# Now we save the trace\n",
    "torch.jit.save(traced_cell, model_savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a961efc-47e6-4757-9bcb-3d590aa865e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate kaggle submission file using the validation script\n",
    "!python {\"validation.py \" + model_savepath + \" --Xts_path \" + Xts_savepath + \" --Xtr_path \" + Xtr_savepath + \" --yts_hat_path \" + yts_hat_savepath } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ba3c3a-aef0-441f-92dc-3c48da1b5937",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
