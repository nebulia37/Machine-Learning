{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "426756a0-4e53-43ae-b822-c6d906c31ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "168c4a57-5d87-42f7-b8df-7cd8e7574a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "Xtr_loadpath = 'Xtr.csv'\n",
    "Xts_loadpath = 'Xts.csv'\n",
    "ytr_loadpath = 'ytr.csv'\n",
    "\n",
    "Xtr = np.loadtxt(Xtr_loadpath, delimiter=\",\")\n",
    "Xts = np.loadtxt(Xts_loadpath, delimiter=\",\")\n",
    "ytr = np.loadtxt(ytr_loadpath, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "44f97d5a-6c11-4b49-83f0-9241dee607a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize the training data\n",
    "Xtr_mean = np.mean(Xtr,axis=0)\n",
    "Xtr_std = np.std(Xtr,axis=0)\n",
    "Xtr_standardized = ((Xtr-Xtr_mean[None,:])/Xtr_std[None,:]) # revise this line as needed\n",
    "Xts_standardized = ((Xts-Xtr_mean[None,:])/Xtr_std[None,:]) # revise this line as needed\n",
    "\n",
    "# save the standardized training data\n",
    "Xtr_savepath = 'Xtr_pytorch.csv'\n",
    "Xts_savepath = 'Xts_pytorch.csv'\n",
    "ytr_savepath = 'ytr_pytorch.csv'\n",
    "yts_hat_savepath = 'yts_hat_pytorch.csv'\n",
    "\n",
    "np.savetxt(Xtr_savepath, Xtr_standardized, delimiter=\",\")\n",
    "np.savetxt(Xts_savepath, Xts_standardized, delimiter=\",\")\n",
    "np.savetxt(ytr_savepath, ytr, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7a52a835-41e4-4669-953a-6863e62834ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the numpy arrays to PyTorch tensors\n",
    "Xtr_torch = torch.Tensor(Xtr_standardized)\n",
    "ytr_torch = torch.Tensor(ytr)\n",
    "#Xts_torch = torch.Tensor(Xts)\n",
    "#yts_torch = torch.Tensor(yts)\n",
    "\n",
    "batch_size = 100  # size of each batch\n",
    "\n",
    "# Create a training Dataset\n",
    "train_ds = torch.utils.data.TensorDataset(Xtr_torch, ytr_torch)\n",
    "# Creates a training DataLoader from this Dataset\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True) \n",
    "\n",
    "# Create a testing Dataset\n",
    "#test_ds = torch.utils.data.TensorDataset(Xts_torch, yts_torch)\n",
    "# Creates a testing DataLoader from this Dataset\n",
    "#test_loader = torch.utils.data.DataLoader(test_ds, batch_size=batch_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "70bc7018-8c43-4976-87d8-30ce1bb5c0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=26, out_features=64, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (7): ReLU()\n",
      "  (8): Linear(in_features=8, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# create a model\n",
    "# d_in = Xtr.shape[1]\n",
    "# d_out = 1\n",
    "\n",
    "# class DumbNet(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(DumbNet, self).__init__()\n",
    "#         self.Dense = nn.Linear(d_in,d_out)\n",
    "#     def forward(self,x):\n",
    "#         out = self.Dense(x)\n",
    "#         return out\n",
    "\n",
    "# model_d = DumbNet()   \n",
    "\n",
    "# Usually, we would train the model at this point. \n",
    "# But this is only a demo, so we'll use the randomly initialized weights.\n",
    "\n",
    "# nin: dimension of input data\n",
    "# nh: number of hidden units\n",
    "# nout: number of outputs\n",
    "\n",
    "# class Net(nn.Module):\n",
    "#     def __init__(self,nin,nh,nout):\n",
    "#         super(Net,self).__init__()\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "#         self.Dense1 = nn.Linear(nin,nh)\n",
    "#         self.Dense2 = nn.Linear(nh,nout)\n",
    "        \n",
    "#     def forward(self,x):\n",
    "#         x = self.sigmoid(self.Dense1(x))\n",
    "#         out = self.Dense2(x)\n",
    "#         return out\n",
    "\n",
    "# model = Net(nin=nin, nh=nh, nout=nout)\n",
    "\n",
    "# print(str(model))\n",
    "\n",
    "# Define the model\n",
    "\n",
    "nin = Xtr.shape[1]\n",
    "nout = 1\n",
    "#nh = 256\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(nin, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16, 8),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(8, nout)\n",
    ")\n",
    "\n",
    "print(str(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "be1fd976-92a5-498f-b00b-5113df5774c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "opt = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "340b76de-aee5-4882-8e63-47c2dbbf3de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # training parameters\n",
    "# n_epochs = 100   # number of epochs to run\n",
    "# batch_size = 10  # size of each batch\n",
    "# batch_start = torch.arange(0, len(X_train), batch_size)\n",
    "\n",
    " \n",
    "# training loop\n",
    "# for epoch in range(n_epochs):\n",
    "#     model.train()\n",
    "#     with tqdm.tqdm(batch_start, unit=\"batch\", mininterval=0, disable=True) as bar:\n",
    "#         bar.set_description(f\"Epoch {epoch}\")\n",
    "#         for start in bar:\n",
    "#             # take a batch\n",
    "#             X_batch = X_train[start:start+batch_size]\n",
    "#             y_batch = y_train[start:start+batch_size]\n",
    "#             # forward pass\n",
    "#             y_pred = model(X_batch)\n",
    "#             loss = loss_fn(y_pred, y_batch)\n",
    "#             # backward pass\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             # update weights\n",
    "#             optimizer.step()\n",
    "#             # print progress\n",
    "#             bar.set_postfix(mse=float(loss))\n",
    "#     # evaluate accuracy at end of each epoch\n",
    "#     model.eval()\n",
    "#     y_pred = model(X_test)\n",
    "#     mse = loss_fn(y_pred, y_test)\n",
    "#     mse = float(mse)\n",
    "#     history.append(mse)\n",
    "#     if mse < best_mse:\n",
    "#         best_mse = mse\n",
    "#         best_weights = copy.deepcopy(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0c8e2b79-660b-4b2b-b6c2-7866bdec7dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1   Train Loss: 90.893   Train Acc: 0.00   \n",
      "Epoch:  2   Train Loss: 80.343   Train Acc: 0.00   \n",
      "Epoch:  3   Train Loss: 76.918   Train Acc: 0.00   \n",
      "Epoch:  4   Train Loss: 75.239   Train Acc: 0.00   \n",
      "Epoch:  5   Train Loss: 73.670   Train Acc: 0.00   \n",
      "Epoch:  6   Train Loss: 72.648   Train Acc: 0.00   \n",
      "Epoch:  7   Train Loss: 71.692   Train Acc: 0.00   \n",
      "Epoch:  8   Train Loss: 71.278   Train Acc: 0.00   \n",
      "Epoch:  9   Train Loss: 70.121   Train Acc: 0.00   \n",
      "Epoch: 10   Train Loss: 69.344   Train Acc: 0.00   \n",
      "Epoch: 11   Train Loss: 68.252   Train Acc: 0.00   \n",
      "Epoch: 12   Train Loss: 67.849   Train Acc: 0.00   \n",
      "Epoch: 13   Train Loss: 66.854   Train Acc: 0.00   \n",
      "Epoch: 14   Train Loss: 66.031   Train Acc: 0.00   \n",
      "Epoch: 15   Train Loss: 65.208   Train Acc: 0.00   \n",
      "Epoch: 16   Train Loss: 64.655   Train Acc: 0.00   \n",
      "Epoch: 17   Train Loss: 64.263   Train Acc: 0.00   \n",
      "Epoch: 18   Train Loss: 63.639   Train Acc: 0.00   \n",
      "Epoch: 19   Train Loss: 62.609   Train Acc: 0.00   \n",
      "Epoch: 20   Train Loss: 62.107   Train Acc: 0.00   \n",
      "Epoch: 21   Train Loss: 62.163   Train Acc: 0.00   \n",
      "Epoch: 22   Train Loss: 60.930   Train Acc: 0.00   \n",
      "Epoch: 23   Train Loss: 60.462   Train Acc: 0.00   \n",
      "Epoch: 24   Train Loss: 60.756   Train Acc: 0.00   \n",
      "Epoch: 25   Train Loss: 59.597   Train Acc: 0.00   \n",
      "Epoch: 26   Train Loss: 58.928   Train Acc: 0.00   \n",
      "Epoch: 27   Train Loss: 57.983   Train Acc: 0.00   \n",
      "Epoch: 28   Train Loss: 57.852   Train Acc: 0.00   \n",
      "Epoch: 29   Train Loss: 56.997   Train Acc: 0.00   \n",
      "Epoch: 30   Train Loss: 56.666   Train Acc: 0.00   \n",
      "Epoch: 31   Train Loss: 56.237   Train Acc: 0.00   \n",
      "Epoch: 32   Train Loss: 56.179   Train Acc: 0.00   \n",
      "Epoch: 33   Train Loss: 55.345   Train Acc: 0.00   \n",
      "Epoch: 34   Train Loss: 55.488   Train Acc: 0.00   \n",
      "Epoch: 35   Train Loss: 54.183   Train Acc: 0.00   \n",
      "Epoch: 36   Train Loss: 53.601   Train Acc: 0.00   \n",
      "Epoch: 37   Train Loss: 52.970   Train Acc: 0.00   \n",
      "Epoch: 38   Train Loss: 53.270   Train Acc: 0.00   \n",
      "Epoch: 39   Train Loss: 52.151   Train Acc: 0.00   \n",
      "Epoch: 40   Train Loss: 52.397   Train Acc: 0.00   \n",
      "Epoch: 41   Train Loss: 51.649   Train Acc: 0.00   \n",
      "Epoch: 42   Train Loss: 52.549   Train Acc: 0.00   \n",
      "Epoch: 43   Train Loss: 50.722   Train Acc: 0.00   \n",
      "Epoch: 44   Train Loss: 50.727   Train Acc: 0.00   \n",
      "Epoch: 45   Train Loss: 50.372   Train Acc: 0.00   \n",
      "Epoch: 46   Train Loss: 49.593   Train Acc: 0.00   \n",
      "Epoch: 47   Train Loss: 49.360   Train Acc: 0.00   \n",
      "Epoch: 48   Train Loss: 48.589   Train Acc: 0.00   \n",
      "Epoch: 49   Train Loss: 48.820   Train Acc: 0.00   \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18072\\831964113.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;31m# Take an optimization 'step'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;31m# Do hard classification: index of largest score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    267\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m             \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 269\u001b[1;33m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    270\u001b[0m                 \u001b[1;31m# call optimizer step pre hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mpre_hook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mchain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_global_optimizer_pre_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_optimizer_step_pre_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\profiler.py\u001b[0m in \u001b[0;36m__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 492\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_record_function_enter_new\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    493\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    500\u001b[0m         \u001b[1;31m# We save the function ptr as the `op` attribute on\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m         \u001b[1;31m# OpOverloadPacket to access it here.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 502\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    503\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m     \u001b[1;31m# TODO: use this to make a __dir__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "num_epoch = 50\n",
    "#batch_size = 10  # size of each batch\n",
    "\n",
    "a_tr_loss = np.zeros([num_epoch])\n",
    "#a_tr_accuracy = np.zeros([num_epoch])\n",
    "#a_ts_loss = np.zeros([num_epoch])\n",
    "#a_ts_accuracy = np.zeros([num_epoch])\n",
    "\n",
    "a_tr_Rsq = np.zeros([num_epoch])\n",
    "\n",
    "for epoch in range(num_epoch):    \n",
    "    model.train() # put model in training mode\n",
    "    \n",
    "    # Initialize variables for tracking loss and R-squared score\n",
    "    epoch_Rsq = 0.0\n",
    "    batch_loss_tr = []\n",
    "    # iterate over training set\n",
    "    for train_iter, data in enumerate(train_loader):\n",
    "        x_batch,y_batch = data\n",
    "        y_batch = y_batch.view(-1,1)\n",
    "        #y_batch = y_batch.type(torch.long)\n",
    "        out = model(x_batch)\n",
    "        # Compute Loss\n",
    "        loss = criterion(out,y_batch.type(torch.float))\n",
    "        batch_loss_tr.append(loss.item())\n",
    "        # Compute gradients using back propagation\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        # Take an optimization 'step'\n",
    "        opt.step()\n",
    "        \n",
    "        # Do hard classification: index of largest score\n",
    "        #predicted = out.clamp(0,1).round().type(torch.long)\n",
    "        # Compute number of decision errors\n",
    "        #total += y_batch.size(0)\n",
    "        #correct += (out == y_batch).sum().item()\n",
    "        #epoch_Rsq += r2_score(y_batch.detach().numpy(), out.detach().numpy())\n",
    "        \n",
    "    epoch_Rsq /= len(train_loader)\n",
    "    a_tr_loss[epoch] = np.mean(batch_loss_tr) # Compute average loss over epoch\n",
    "    a_tr_Rsq[epoch] = epoch_Rsq\n",
    "    \n",
    "#     model.eval() # put model in evaluation mode\n",
    "#     correct = 0 # initialize error counter\n",
    "#     total = 0 # initialize total counter\n",
    "#     batch_loss_ts = []\n",
    "#     with torch.no_grad():\n",
    "#         for data in test_loader:\n",
    "#             images, labels = data\n",
    "#             labels = labels.type(torch.long)\n",
    "#             outputs = model(images)\n",
    "#             batch_loss_ts.append(criterion(outputs,labels).item())\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "            \n",
    "#     a_ts_loss[epoch] = np.mean(batch_loss_ts)\n",
    "#     a_ts_accuracy[epoch] = 100*correct/total\n",
    "    \n",
    "    # Print details every print_mod epoch\n",
    "    print('Epoch: {0:2d}   Train Loss: {1:.3f}   '.format(epoch+1, a_tr_loss[epoch])\n",
    "          +'Train Acc: {0:.2f}   '.format(a_tr_Rsq[epoch]))\n",
    "    \n",
    "    # print('Epoch: {0:2d}   Train Loss: {1:.3f}   '.format(epoch+1, a_tr_loss[epoch])\n",
    "    #       +'Train Acc: {0:.2f}    Test Loss: {1:.3f}   '.format(a_tr_accuracy[epoch], a_ts_loss[epoch])\n",
    "    #       +'Test Acc: {0:.2f}'.format(a_ts_accuracy[epoch]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2de6e8-665e-4c47-a689-14e796453a29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
